# None Trainer

<div align="center">

![Logo](https://img.shields.io/badge/None-Trainer-f0b429?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiMxYTFhMWQiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJtMTIgMyA4IDR2NmMwIDUuNTMtMy42MSA4Ljk5LTggMTEtNC4zOS0yLjAxLTgtNS40Ny04LTExVjdsMTItNFoiLz48L3N2Zz4=)

**Z-Image Turbo LoRA è®­ç»ƒå·¥ä½œå®¤**

åŸºäº **AC-RFï¼ˆé”šç‚¹è€¦åˆæ•´æµæµï¼‰** ç®—æ³•çš„é«˜æ•ˆ LoRA å¾®è°ƒå·¥å…·

</div>

---

## âœ¨ ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| ğŸ¯ **é”šç‚¹è€¦åˆé‡‡æ ·** | åªåœ¨å…³é”®æ—¶é—´æ­¥è®­ç»ƒï¼Œé«˜æ•ˆç¨³å®š |
| âš¡ **10æ­¥å¿«é€Ÿæ¨ç†** | ä¿æŒ Turbo æ¨¡å‹çš„åŠ é€Ÿç»“æ„ |
| ğŸ“‰ **Min-SNR åŠ æƒ** | å‡å°‘ä¸åŒæ—¶é—´æ­¥çš„ loss æ³¢åŠ¨ |
| ğŸ”§ **è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–** | æ£€æµ‹ GPU å¹¶è‡ªåŠ¨é…ç½® (Tier S/A/B) |
| ğŸ–¥ï¸ **ç°ä»£åŒ– WebUI** | Vue.js + FastAPI å…¨æ ˆç•Œé¢ |
| ğŸ“Š **å®æ—¶ç›‘æ§** | Loss æ›²çº¿ã€è¿›åº¦ã€æ˜¾å­˜ç›‘æ§ |
| ğŸ·ï¸ **Ollama æ ‡æ³¨** | ä¸€é”® AI å›¾ç‰‡æ‰“æ ‡ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Step 1: å®‰è£… PyTorchï¼ˆå¿…é¡»ï¼‰

æ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©ï¼š

```bash
# CUDA 12.8 (RTX 40ç³»åˆ—æ¨è)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# CUDA 12.4
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8 (æ—§æ˜¾å¡)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Step 2: å®‰è£… Flash Attentionï¼ˆæ¨èï¼‰

Flash Attention å¯æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿè®­ç»ƒã€‚

**Linux** - ä» [Flash Attention Releases](https://github.com/Dao-AILab/flash-attention/releases) ä¸‹è½½ï¼š

```bash
# æŸ¥çœ‹ä½ çš„ç¯å¢ƒç‰ˆæœ¬
python --version                                      # ä¾‹å¦‚: Python 3.12
python -c "import torch; print(torch.version.cuda)"  # ä¾‹å¦‚: 12.8

# ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ï¼ˆç¤ºä¾‹ï¼šPython 3.12 + CUDA 12 + PyTorch 2.5ï¼‰
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl

# å®‰è£…
pip install flash_attn-*.whl
```

**Windows** - ä» [AI-windows-whl](https://huggingface.co/Wildminder/AI-windows-whl/tree/main) ä¸‹è½½é¢„ç¼–è¯‘ç‰ˆï¼š

```batch
:: ç¤ºä¾‹ï¼šPython 3.12 + CUDA 12.8 + PyTorch 2.9.1
pip install https://huggingface.co/Wildminder/AI-windows-whl/resolve/main/flash_attn-2.8.3+cu128torch2.9.1cxx11abiTRUE-cp313-cp313-win_amd64.whl

:: æˆ–ä¸‹è½½åæœ¬åœ°å®‰è£…
pip install flash_attn-xxx.whl
```

> **æç¤º**: å¦‚æœæ²¡æœ‰å¯¹åº”ç‰ˆæœ¬ï¼Œå¯è·³è¿‡æ­¤æ­¥ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨ SDPA ä½œä¸ºå¤‡é€‰ã€‚

### Step 3: å®‰è£… Diffusersï¼ˆå¿…é¡»ï¼‰

âš ï¸ **æ³¨æ„**: æœ¬é¡¹ç›®éœ€è¦ diffusers 0.36+ï¼ˆå¼€å‘ç‰ˆï¼‰ï¼Œpip æš‚æ— å‘å¸ƒï¼Œéœ€ä» git å®‰è£…ï¼š

```bash
pip install git+https://github.com/huggingface/diffusers.git
```

### Step 4: ä¸€é”®éƒ¨ç½²

#### Linux / Mac

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/None9527/None_Z-image-Turbo_trainer.git
cd None_Z-image-Turbo_trainer

# ä¸€é”®å®‰è£…ä¾èµ–
chmod +x setup.sh
./setup.sh

# ç¼–è¾‘é…ç½®ï¼ˆè®¾ç½®æ¨¡å‹è·¯å¾„ï¼‰
cp env.example .env
nano .env

# å¯åŠ¨æœåŠ¡
./start.sh
```

#### Windows

```batch
:: å…‹éš†é¡¹ç›®
git clone https://github.com/None9527/None_Z-image-Turbo_trainer.git
cd None_Z-image-Turbo_trainer

:: ä¸€é”®å®‰è£…ä¾èµ–ï¼ˆåŒå‡»æˆ–å‘½ä»¤è¡Œï¼‰
setup.bat

:: ç¼–è¾‘é…ç½®ï¼ˆè®¾ç½®æ¨¡å‹è·¯å¾„ï¼‰
copy env.example .env
notepad .env

:: å¯åŠ¨æœåŠ¡
start.bat
```

### Step 5: è®¿é—® Web UI

éƒ¨ç½²å®Œæˆåæ‰“å¼€æµè§ˆå™¨è®¿é—®: **http://localhost:9198**

---

## ğŸ“¦ æ‰‹åŠ¨å®‰è£…ï¼ˆå¯é€‰ï¼‰

<details>
<summary>å¦‚æœä¸€é”®éƒ¨ç½²é‡åˆ°é—®é¢˜ï¼Œå¯å±•å¼€æ‰‹åŠ¨å®‰è£…</summary>

```bash
# 1. å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# 2. å®‰è£… diffusers æœ€æ–°ç‰ˆ
pip install git+https://github.com/huggingface/diffusers.git

# 3. å®‰è£…æœ¬é¡¹ç›®
pip install -e .

# 4. åˆ›å»ºé…ç½®æ–‡ä»¶
cp env.example .env

# 5. å¯åŠ¨æœåŠ¡
cd webui-vue/api && python main.py --port 9198
```

</details>

---

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡ (`.env`)

```bash
# æœåŠ¡é…ç½®
TRAINER_PORT=9198           # Web UI ç«¯å£
TRAINER_HOST=0.0.0.0        # ç›‘å¬åœ°å€

# æ¨¡å‹è·¯å¾„
MODEL_PATH=/./zimage_models

# æ•°æ®é›†è·¯å¾„
DATASET_PATH=./datasets

# Ollama é…ç½®
OLLAMA_HOST=http://127.0.0.1:11434
```

### è®­ç»ƒå‚æ•° (`config/acrf_config.toml`)

```toml
[acrf]
turbo_steps = 10        # é”šç‚¹æ•°ï¼ˆæ¨ç†æ­¥æ•°ï¼‰
shift = 3.0             # Z-Image å®˜æ–¹å€¼
jitter_scale = 0.02     # é”šç‚¹æŠ–åŠ¨

[lora]
network_dim = 16        # LoRA rank
network_alpha = 16      # LoRA alpha

[training]
learning_rate = 1e-4    # å­¦ä¹ ç‡
num_train_epochs = 10   # è®­ç»ƒè½®æ•°
snr_gamma = 5.0         # Min-SNR åŠ æƒ
lambda_fft = 0          # FFT é¢‘åŸŸ loss æƒé‡
lambda_cosine = 0       # Cosine ç›¸ä¼¼åº¦ loss æƒé‡
```

> âš ï¸ **è­¦å‘Š**: `lambda_fft` å’Œ `lambda_cosine` æ˜¯å®éªŒæ€§çš„æ··åˆ loss å‚æ•°ã€‚**å¦‚æœä½ ä¸äº†è§£å®ƒä»¬çš„ä½œç”¨ï¼Œè¯·ä¿æŒä¸º 0ï¼Œä¸è¦å¼€å¯ï¼** é”™è¯¯çš„è®¾ç½®å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šæˆ–æ•ˆæœå˜å·®ã€‚

### ç¡¬ä»¶åˆ†çº§

| Tier | æ˜¾å­˜ | æ˜¾å¡ç¤ºä¾‹ | è‡ªåŠ¨ä¼˜åŒ–ç­–ç•¥ |
|------|------|----------|-------------|
| **S** | 32GB+ | A100/H100/5090 | å…¨æ€§èƒ½ï¼Œæ— å‹ç¼© |
| **A** | 24GB | 3090/4090 | é«˜æ€§èƒ½ï¼ŒåŸç”Ÿ SDPA |
| **B** | 16GB | 4080/4070Ti | å¹³è¡¡æ¨¡å¼ï¼Œè½»åº¦å‹ç¼© |

---

## ğŸ“Š ä½¿ç”¨æµç¨‹

| æ­¥éª¤ | åŠŸèƒ½ | è¯´æ˜ |
|:---:|:---:|:---|
| 1ï¸âƒ£ | **æ•°æ®é›†** | å¯¼å…¥å›¾ç‰‡ã€Ollama AI æ ‡æ³¨ |
| â¡ï¸ | | |
| 2ï¸âƒ£ | **ç¼“å­˜** | é¢„è®¡ç®— Latent å’Œ Text åµŒå…¥ |
| â¡ï¸ | | |
| 3ï¸âƒ£ | **è®­ç»ƒ** | AC-RF LoRA å¾®è°ƒ |
| â¡ï¸ | | |
| 4ï¸âƒ£ | **ç”Ÿæˆ** | åŠ è½½ LoRA æµ‹è¯•æ•ˆæœ |

---

## ğŸ”§ å¸¸è§é—®é¢˜

<details>
<summary><strong>Q: loss è·³åŠ¨å¾ˆå¤§ï¼ˆ0.08-0.6ï¼‰ï¼Ÿ</strong></summary>

A: æ­£å¸¸ç°è±¡ï¼ä¸åŒ sigma ä¸‹é¢„æµ‹éš¾åº¦ä¸åŒã€‚çœ‹ **EMA loss** æ˜¯å¦æ•´ä½“ä¸‹é™å³å¯ã€‚

</details>

<details>
<summary><strong>Q: CUDA Out of Memoryï¼Ÿ</strong></summary>

A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
- å¢å¤§ `gradient_accumulation_steps`ï¼ˆå¦‚ 4 â†’ 8ï¼‰
- é™ä½ `network_dim`ï¼ˆå¦‚ 32 â†’ 16ï¼‰
- ç¡®ä¿å·²å®‰è£… Flash Attention

</details>

<details>
<summary><strong>Q: è®­ç»ƒå¤šå°‘ epochï¼Ÿ</strong></summary>

A: å–å†³äºæ•°æ®é›†å¤§å°ï¼š
- < 50 å¼ ï¼š10-15 epoch
- 50-200 å¼ ï¼š8-10 epoch
- \> 200 å¼ ï¼š5-8 epoch

</details>

---

## ğŸ“¬ è”ç³»æ–¹å¼

- ğŸ“§ lihaonan1082@gmail.com
- ğŸ“® 592532681@qq.com

---

## ğŸ“ License

Apache 2.0

## ğŸ™ è‡´è°¢

- [Z-Image](https://github.com/Alpha-VLLM/Lumina-Image) - åŸºç¡€æ¨¡å‹
- [diffusers](https://github.com/huggingface/diffusers) - è®­ç»ƒæ¡†æ¶
- [Flash Attention](https://github.com/Dao-AILab/flash-attention) - é«˜æ•ˆæ³¨æ„åŠ›
  
---

<div align="center">

**Made with â¤ï¸ by None**

</div>
