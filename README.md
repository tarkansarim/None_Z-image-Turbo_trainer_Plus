# None Trainer

<div align="center">

![Logo](https://img.shields.io/badge/None-Trainer-f0b429?style=for-the-badge&logo=data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9IiMxYTFhMWQiIHN0cm9rZS13aWR0aD0iMiIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIiBzdHJva2UtbGluZWpvaW49InJvdW5kIj48cGF0aCBkPSJtMTIgMyA4IDR2NmMwIDUuNTMtMy42MSA4Ljk5LTggMTEtNC4zOS0yLjAxLTgtNS40Ny04LTExVjdsMTItNFoiLz48L3N2Zz4=)

**Z-Image Turbo LoRA è®­ç»ƒå·¥ä½œå®¤**

åŸºäº **AC-RFï¼ˆé”šç‚¹è€¦åˆæ•´æµæµï¼‰** ç®—æ³•çš„é«˜æ•ˆ LoRA å¾®è°ƒå·¥å…·

</div>

---

## âœ¨ ç‰¹æ€§

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| ğŸ¯ **é”šç‚¹è€¦åˆé‡‡æ ·** | åªåœ¨å…³é”®æ—¶é—´æ­¥è®­ç»ƒï¼Œé«˜æ•ˆç¨³å®š |
| âš¡ **10æ­¥å¿«é€Ÿæ¨ç†** | ä¿æŒ Turbo æ¨¡å‹çš„åŠ é€Ÿç»“æ„ |
| ğŸ“‰ **Min-SNR åŠ æƒ** | å‡å°‘ä¸åŒæ—¶é—´æ­¥çš„ loss æ³¢åŠ¨ |
| ğŸ¨ **å¤šç§æŸå¤±æ¨¡å¼** | é¢‘åŸŸæ„ŸçŸ¥ / é£æ ¼ç»“æ„ / ç»Ÿä¸€æ¨¡å¼ |
| ğŸ”§ **è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–** | æ£€æµ‹ GPU å¹¶è‡ªåŠ¨é…ç½® (Tier S/A/B) |
| ğŸ–¥ï¸ **ç°ä»£åŒ– WebUI** | Vue.js + FastAPI å…¨æ ˆç•Œé¢ |
| ğŸ“Š **å®æ—¶ç›‘æ§** | Loss æ›²çº¿ã€è¿›åº¦ã€æ˜¾å­˜ç›‘æ§ |
| ğŸ·ï¸ **Ollama æ ‡æ³¨** | ä¸€é”® AI å›¾ç‰‡æ‰“æ ‡ |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Step 1: å®‰è£… PyTorchï¼ˆå¿…é¡»ï¼‰

æ ¹æ®ä½ çš„ CUDA ç‰ˆæœ¬é€‰æ‹©ï¼š

```bash
# CUDA 12.8 (RTX 40ç³»åˆ—æ¨è)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128

# CUDA 12.4
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# CUDA 11.8 (æ—§æ˜¾å¡)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Step 2: å®‰è£… Flash Attentionï¼ˆæ¨èï¼‰

Flash Attention å¯æ˜¾è‘—é™ä½æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿè®­ç»ƒã€‚

**Linux** - ä» [Flash Attention Releases](https://github.com/Dao-AILab/flash-attention/releases) ä¸‹è½½ï¼š

```bash
# æŸ¥çœ‹ä½ çš„ç¯å¢ƒç‰ˆæœ¬
python --version                                      # ä¾‹å¦‚: Python 3.12
python -c "import torch; print(torch.version.cuda)"  # ä¾‹å¦‚: 12.8

# ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ï¼ˆç¤ºä¾‹ï¼šPython 3.12 + CUDA 12 + PyTorch 2.5ï¼‰
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl

# å®‰è£…
pip install flash_attn-*.whl
```

**Windows** - ä» [AI-windows-whl](https://huggingface.co/Wildminder/AI-windows-whl/tree/main) ä¸‹è½½é¢„ç¼–è¯‘ç‰ˆï¼š

```batch
:: ç¤ºä¾‹ï¼šPython 3.12 + CUDA 12.8 + PyTorch 2.9.1
pip install https://huggingface.co/Wildminder/AI-windows-whl/resolve/main/flash_attn-2.8.3+cu128torch2.9.1cxx11abiTRUE-cp313-cp313-win_amd64.whl

:: æˆ–ä¸‹è½½åæœ¬åœ°å®‰è£…
pip install flash_attn-xxx.whl
```

> **æç¤º**: å¦‚æœæ²¡æœ‰å¯¹åº”ç‰ˆæœ¬ï¼Œå¯è·³è¿‡æ­¤æ­¥ï¼Œç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨ SDPA ä½œä¸ºå¤‡é€‰ã€‚

### Step 3: å®‰è£… Diffusersï¼ˆå¿…é¡»ï¼‰

âš ï¸ **æ³¨æ„**: æœ¬é¡¹ç›®éœ€è¦ diffusers 0.36+ï¼ˆå¼€å‘ç‰ˆï¼‰ï¼Œpip æš‚æ— å‘å¸ƒï¼Œéœ€ä» git å®‰è£…ï¼š

```bash
pip install git+https://github.com/huggingface/diffusers.git
```

### Step 4: ä¸€é”®éƒ¨ç½²

#### Linux / Mac

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/None9527/None_Z-image-Turbo_trainer.git
cd None_Z-image-Turbo_trainer

# ä¸€é”®å®‰è£…ä¾èµ–
chmod +x setup.sh
./setup.sh

# ç¼–è¾‘é…ç½®ï¼ˆè®¾ç½®æ¨¡å‹è·¯å¾„ï¼‰
cp env.example .env
nano .env

# å¯åŠ¨æœåŠ¡
./start.sh
```

#### Windows

```batch
:: å…‹éš†é¡¹ç›®
git clone https://github.com/None9527/None_Z-image-Turbo_trainer.git
cd None_Z-image-Turbo_trainer

:: ä¸€é”®å®‰è£…ä¾èµ–ï¼ˆåŒå‡»æˆ–å‘½ä»¤è¡Œï¼‰
setup.bat

:: ç¼–è¾‘é…ç½®ï¼ˆè®¾ç½®æ¨¡å‹è·¯å¾„ï¼‰
copy env.example .env
notepad .env

:: å¯åŠ¨æœåŠ¡
start.bat
```

### Step 5: è®¿é—® Web UI

éƒ¨ç½²å®Œæˆåæ‰“å¼€æµè§ˆå™¨è®¿é—®: **http://localhost:9198**

---

## ğŸ“¦ æ‰‹åŠ¨å®‰è£…ï¼ˆå¯é€‰ï¼‰

<details>
<summary>å¦‚æœä¸€é”®éƒ¨ç½²é‡åˆ°é—®é¢˜ï¼Œå¯å±•å¼€æ‰‹åŠ¨å®‰è£…</summary>

```bash
# 1. å®‰è£… Python ä¾èµ–
pip install -r requirements.txt

# 2. å®‰è£… diffusers æœ€æ–°ç‰ˆ
pip install git+https://github.com/huggingface/diffusers.git

# 3. å®‰è£…æœ¬é¡¹ç›®
pip install -e .

# 4. åˆ›å»ºé…ç½®æ–‡ä»¶
cp env.example .env

# 5. å¯åŠ¨æœåŠ¡
cd webui-vue/api && python main.py --port 9198
```

</details>

---

## ğŸ–¥ï¸ å‘½ä»¤è¡Œä½¿ç”¨ï¼ˆé«˜çº§ï¼‰

é™¤äº† Web UIï¼Œä½ ä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨å‘½ä»¤è¡Œè¿›è¡Œæ“ä½œï¼š

### ç”Ÿæˆç¼“å­˜

```bash
# ç”Ÿæˆ Latent ç¼“å­˜ï¼ˆVAE ç¼–ç ï¼‰
python -m zimage_trainer.cache_latents \
    --model_path ./zimage_models \
    --dataset_path ./datasets/your_dataset \
    --output_dir ./datasets/your_dataset

# ç”Ÿæˆ Text ç¼“å­˜ï¼ˆæ–‡æœ¬ç¼–ç ï¼‰
python -m zimage_trainer.cache_text_encoder \
    --model_path ./zimage_models \
    --dataset_path ./datasets/your_dataset \
    --output_dir ./datasets/your_dataset
```

### å¯åŠ¨è®­ç»ƒ

```bash
# ä½¿ç”¨é…ç½®æ–‡ä»¶è®­ç»ƒï¼ˆæ¨èï¼‰
python scripts/train_acrf.py --config config/acrf_config.toml

# æŒ‡å®šæŸå¤±æ¨¡å¼
python scripts/train_acrf.py --config config/acrf_config.toml --loss_mode frequency

# é¢‘åŸŸæ„ŸçŸ¥æ¨¡å¼ + è‡ªå®šä¹‰å‚æ•°
python scripts/train_acrf.py --config config/acrf_config.toml \
    --loss_mode frequency \
    --alpha_hf 1.0 \
    --beta_lf 0.2

# é£æ ¼ç»“æ„æ¨¡å¼
python scripts/train_acrf.py --config config/acrf_config.toml \
    --loss_mode style \
    --lambda_struct 1.0 \
    --lambda_light 0.5 \
    --lambda_color 0.3

# ç»Ÿä¸€æ¨¡å¼ï¼ˆé¢‘åŸŸ + é£æ ¼ï¼‰
python scripts/train_acrf.py --config config/acrf_config.toml --loss_mode unified
```

### æ¨ç†ç”Ÿæˆ

```bash
# åŠ è½½ LoRA ç”Ÿæˆå›¾ç‰‡
python -m zimage_trainer.inference \
    --model_path ./zimage_models \
    --lora_path ./output/your_lora.safetensors \
    --prompt "your prompt here" \
    --output_path ./output/generated.png \
    --num_inference_steps 10
```

### å¯åŠ¨ Web UI æœåŠ¡

```bash
# æ–¹å¼ä¸€ï¼šä½¿ç”¨è„šæœ¬
./start.sh          # Linux/Mac
start.bat           # Windows

# æ–¹å¼äºŒï¼šç›´æ¥å¯åŠ¨
cd webui-vue/api
python main.py --port 9198 --host 0.0.0.0

# æ–¹å¼ä¸‰ï¼šä½¿ç”¨ uvicornï¼ˆæ”¯æŒçƒ­é‡è½½ï¼‰
cd webui-vue/api
uvicorn main:app --port 9198 --reload
```

### è½¬æ¢ LoRA æ ¼å¼

```bash
# è½¬æ¢ä¸º ComfyUI å…¼å®¹æ ¼å¼
python scripts/convert_lora_comfyui.py \
    --input ./output/your_lora.safetensors \
    --output ./output/your_lora_comfyui.safetensors
```

---

## âš™ï¸ é…ç½®è¯´æ˜

### ç¯å¢ƒå˜é‡ (`.env`)

```bash
# æœåŠ¡é…ç½®
TRAINER_PORT=9198           # Web UI ç«¯å£
TRAINER_HOST=0.0.0.0        # ç›‘å¬åœ°å€

# æ¨¡å‹è·¯å¾„
MODEL_PATH=/./zimage_models

# æ•°æ®é›†è·¯å¾„
DATASET_PATH=./datasets

# Ollama é…ç½®
OLLAMA_HOST=http://127.0.0.1:11434
```

### è®­ç»ƒå‚æ•° (`config/acrf_config.toml`)

```toml
[acrf]
turbo_steps = 10        # é”šç‚¹æ•°ï¼ˆæ¨ç†æ­¥æ•°ï¼‰
shift = 3.0             # Z-Image å®˜æ–¹å€¼
jitter_scale = 0.02     # é”šç‚¹æŠ–åŠ¨

[lora]
network_dim = 16        # LoRA rank
network_alpha = 16      # LoRA alpha

[training]
learning_rate = 1e-4    # å­¦ä¹ ç‡
num_train_epochs = 10   # è®­ç»ƒè½®æ•°
snr_gamma = 5.0         # Min-SNR åŠ æƒ
loss_mode = "standard"  # æŸå¤±æ¨¡å¼ï¼ˆè§ä¸‹æ–¹è¯´æ˜ï¼‰
```

### ğŸ¨ æŸå¤±æ¨¡å¼ (Loss Mode)

æ–°ç‰ˆæœ¬æ”¯æŒ 4 ç§æŸå¤±æ¨¡å¼ï¼Œå¯åœ¨å‰ç«¯"é«˜çº§é€‰é¡¹"ä¸­é€‰æ‹©ï¼š

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ | æ¨èå‚æ•° |
|------|------|----------|----------|
| **standard** | åŸºç¡€ MSE + å¯é€‰ FFT/Cosine | é€šç”¨è®­ç»ƒ | é»˜è®¤å³å¯ |
| **frequency** | é¢‘åŸŸæ„ŸçŸ¥ï¼ˆé«˜é¢‘L1 + ä½é¢‘Cosineï¼‰ | é”åŒ–ç»†èŠ‚ï¼Œä¸æ”¹é£æ ¼ | alpha_hf=1.0, beta_lf=0.2 |
| **style** | é£æ ¼ç»“æ„ï¼ˆSSIM + Labç»Ÿè®¡é‡ï¼‰ | å­¦ä¹ å¤§å¸ˆå…‰å½±/è°ƒè‰² | lambda_struct=1.0 |
| **unified** | é¢‘åŸŸ + é£æ ¼ ç»„åˆ | å…¨é¢å¢å¼º | ä¸¤è€…é»˜è®¤å€¼ |

#### é¢‘åŸŸæ„ŸçŸ¥æ¨¡å¼ (frequency)

```
æ ¸å¿ƒåŸç†ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Latent â”€â”€â–º é™é‡‡æ · â”€â”€â–º ä½é¢‘ï¼ˆç»“æ„ï¼‰     â”‚
â”‚         â””â”€â”€â–º é«˜é¢‘ = åŸå§‹ - ä½é¢‘ï¼ˆç»†èŠ‚ï¼‰ â”‚
â”‚                                         â”‚
â”‚  Loss = MSE + Î±Â·L1(é«˜é¢‘) + Î²Â·Cos(ä½é¢‘)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å‚æ•°è¯´æ˜ï¼š
- alpha_hf: é«˜é¢‘å¢å¼ºæƒé‡ï¼ˆâ†‘é”åŒ– â†‘å™ªç‚¹é£é™©ï¼‰æ¨è 0.5~1.0
- beta_lf:  ä½é¢‘é”å®šæƒé‡ï¼ˆâ†‘ä¿æŒç»“æ„ï¼‰æ¨è 0.1~0.3
```

#### é£æ ¼ç»“æ„æ¨¡å¼ (style)

```
æ ¸å¿ƒåŸç†ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Latent è¿‘ä¼¼ Lab ç©ºé—´                        â”‚
â”‚  â”œâ”€ Lé€šé“ â”€â”€â–º SSIMï¼ˆé”ç»“æ„ï¼‰                â”‚
â”‚  â”‚         â”œâ”€ Mean/Stdï¼ˆå­¦å…‰å½±ï¼‰            â”‚
â”‚  â”‚         â””â”€ é«˜é¢‘L1ï¼ˆå­¦çº¹ç†ï¼‰              â”‚
â”‚  â””â”€ abé€šé“ â”€â”€â–º Mean/Stdï¼ˆå­¦è‰²è°ƒï¼‰           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å‚æ•°è¯´æ˜ï¼š
- lambda_struct: ç»“æ„é”ï¼ˆé˜²è„¸å´©ï¼‰æ¨è 0.5~1.0
- lambda_light:  å…‰å½±å­¦ä¹ ï¼ˆå­¦Sæ›²çº¿ï¼‰æ¨è 0.3~0.8
- lambda_color:  è‰²è°ƒè¿ç§»ï¼ˆå­¦å†·æš–è°ƒï¼‰æ¨è 0.2~0.5
- lambda_tex:    è´¨æ„Ÿå¢å¼ºï¼ˆå­¦é¢—ç²’æ„Ÿï¼‰æ¨è 0.3~0.5
```

#### é€‰æ‹©å»ºè®®

| ä½ çš„ç›®æ ‡ | æ¨èæ¨¡å¼ |
|----------|----------|
| è®­ç»ƒäººç‰©/è§’è‰² LoRA | `standard` |
| æå‡ç”»é¢æ¸…æ™°åº¦ | `frequency` |
| å­¦ä¹ ç‰¹å®šæ‘„å½±å¸ˆé£æ ¼ | `style` |
| å…¨é¢æå‡è´¨é‡ | `unified` |

> ğŸ’¡ **æ–°æ‰‹å»ºè®®**ï¼šå…ˆç”¨ `standard` æ¨¡å¼è®­ç»ƒï¼Œæ•ˆæœä¸æ»¡æ„å†å°è¯•å…¶ä»–æ¨¡å¼ã€‚

### ç¡¬ä»¶åˆ†çº§

| Tier | æ˜¾å­˜ | æ˜¾å¡ç¤ºä¾‹ | è‡ªåŠ¨ä¼˜åŒ–ç­–ç•¥ |
|------|------|----------|-------------|
| **S** | 32GB+ | A100/H100/5090 | å…¨æ€§èƒ½ï¼Œæ— å‹ç¼© |
| **A** | 24GB | 3090/4090 | é«˜æ€§èƒ½ï¼ŒåŸç”Ÿ SDPA |
| **B** | 16GB | 4080/4070Ti | å¹³è¡¡æ¨¡å¼ï¼Œè½»åº¦å‹ç¼© |

---

## ğŸ“Š ä½¿ç”¨æµç¨‹

| æ­¥éª¤ | åŠŸèƒ½ | è¯´æ˜ |
|:---:|:---:|:---|
| 1ï¸âƒ£ | **æ•°æ®é›†** | å¯¼å…¥å›¾ç‰‡ã€Ollama AI æ ‡æ³¨ |
| â¡ï¸ | | |
| 2ï¸âƒ£ | **ç¼“å­˜** | é¢„è®¡ç®— Latent å’Œ Text åµŒå…¥ |
| â¡ï¸ | | |
| 3ï¸âƒ£ | **è®­ç»ƒ** | AC-RF LoRA å¾®è°ƒ |
| â¡ï¸ | | |
| 4ï¸âƒ£ | **ç”Ÿæˆ** | åŠ è½½ LoRA æµ‹è¯•æ•ˆæœ |

---

## ğŸ”§ å¸¸è§é—®é¢˜

<details>
<summary><strong>Q: loss è·³åŠ¨å¾ˆå¤§ï¼ˆ0.08-0.6ï¼‰ï¼Ÿ</strong></summary>

A: æ­£å¸¸ç°è±¡ï¼ä¸åŒ sigma ä¸‹é¢„æµ‹éš¾åº¦ä¸åŒã€‚çœ‹ **EMA loss** æ˜¯å¦æ•´ä½“ä¸‹é™å³å¯ã€‚

</details>

<details>
<summary><strong>Q: CUDA Out of Memoryï¼Ÿ</strong></summary>

A: å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š
- å¢å¤§ `gradient_accumulation_steps`ï¼ˆå¦‚ 4 â†’ 8ï¼‰
- é™ä½ `network_dim`ï¼ˆå¦‚ 32 â†’ 16ï¼‰
- ç¡®ä¿å·²å®‰è£… Flash Attention

</details>

<details>
<summary><strong>Q: è®­ç»ƒå¤šå°‘ epochï¼Ÿ</strong></summary>

A: å–å†³äºæ•°æ®é›†å¤§å°ï¼š
- < 50 å¼ ï¼š10-15 epoch
- 50-200 å¼ ï¼š8-10 epoch
- \> 200 å¼ ï¼š5-8 epoch

</details>

---

## ğŸ“¬ è”ç³»æ–¹å¼

- ğŸ“§ lihaonan1082@gmail.com
- ğŸ“® 592532681@qq.com

---

## ğŸ“ License

Apache 2.0

## ğŸ™ è‡´è°¢

- [Z-Image](https://github.com/Alpha-VLLM/Lumina-Image) - åŸºç¡€æ¨¡å‹
- [diffusers](https://github.com/huggingface/diffusers) - è®­ç»ƒæ¡†æ¶
- [Flash Attention](https://github.com/Dao-AILab/flash-attention) - é«˜æ•ˆæ³¨æ„åŠ›
  
---

<div align="center">

**Made with â¤ï¸ by None**

</div>
