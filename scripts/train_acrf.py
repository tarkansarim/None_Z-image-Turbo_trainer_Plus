"""
[START] AC-RF Training Script for Z-Image-Turbo

ç‹¬ç«‹çš„ Anchor-Constrained Rectified Flow è®­ç»ƒè„šæœ¬
ç”¨äº Z-Image-Turbo æ¨¡å‹çš„ LoRA å¾®è°ƒå®éªŒ

å…³é”®ç‰¹æ€§ï¼š
- ä¿æŒ Turbo æ¨¡å‹çš„ç›´çº¿åŠ é€Ÿç»“æ„
- åªåœ¨å…³é”®é”šç‚¹æ—¶é—´æ­¥è®­ç»ƒ
- ç›´æ¥å›å½’é€Ÿåº¦å‘é‡è€Œéé¢„æµ‹å™ªå£°
"""

import os
import sys
import math
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../src"))

import torch
import argparse
from pathlib import Path
from tqdm import tqdm
from accelerate import Accelerator
from accelerate.utils import set_seed

from zimage_trainer.acrf_trainer import ACRFTrainer
from zimage_trainer.utils.zimage_utils import load_transformer
from zimage_trainer.networks.lora import LoRANetwork
from zimage_trainer.dataset.dataloader import create_dataloader
from zimage_trainer.utils.memory_optimizer import MemoryOptimizer
from zimage_trainer.utils.hardware_detector import HardwareDetector

import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def parse_args():
    parser = argparse.ArgumentParser(description="AC-RF è®­ç»ƒè„šæœ¬")
    
    # é…ç½®æ–‡ä»¶å‚æ•°
    parser.add_argument("--config", type=str, help="è¶…å‚æ•°é…ç½®æ–‡ä»¶è·¯å¾„ (.toml)")
    
    # æ¨¡å‹è·¯å¾„
    parser.add_argument("--dit", type=str, help="Transformer æ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset_config", type=str, help="æ•°æ®é›†é…ç½®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="output/acrf", help="è¾“å‡ºç›®å½•")
    
    # AC-RF å‚æ•°
    parser.add_argument("--turbo_steps", type=int, default=10, help="Turbo æ­¥æ•°ï¼ˆé”šç‚¹æ•°é‡ï¼‰")
    parser.add_argument("--shift", type=float, default=3.0, help="æ—¶é—´æ­¥ shift å‚æ•°")
    parser.add_argument("--jitter_scale", type=float, default=0.02, help="é”šç‚¹æŠ–åŠ¨å¹…åº¦")
    
    # LoRA å‚æ•°
    parser.add_argument("--network_dim", type=int, default=8, help="LoRA rank")
    parser.add_argument("--network_alpha", type=float, default=4.0, help="LoRA alpha")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--optimizer_type", type=str, default="AdamW", choices=["AdamW", "AdamW8bit", "Adafactor"], help="ä¼˜åŒ–å™¨ç±»å‹")
    # Adafactor ç‰¹æœ‰å‚æ•°
    parser.add_argument("--adafactor_scale", action="store_true", help="Adafactor scale_parameter")
    parser.add_argument("--adafactor_relative", action="store_true", help="Adafactor relative_step")
    parser.add_argument("--adafactor_warmup", action="store_true", help="Adafactor warmup_init")
    
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--weight_decay", type=float, default=1e-2, help="æƒé‡è¡°å‡")
    
    # LR Scheduler å‚æ•°
    parser.add_argument("--lr_scheduler", type=str, default="constant", 
        choices=["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"],
        help="å­¦ä¹ ç‡è°ƒåº¦å™¨"
    )
    parser.add_argument("--lr_warmup_steps", type=int, default=0, help="Warmup æ­¥æ•°")
    parser.add_argument("--lr_num_cycles", type=int, default=1, help="Cosine è°ƒåº¦å™¨çš„å¾ªç¯æ¬¡æ•°")
    
    parser.add_argument("--lambda_fft", type=float, default=0.1, help="FFT Loss æƒé‡")
    parser.add_argument("--lambda_cosine", type=float, default=0.1, help="Cosine Loss æƒé‡")
    parser.add_argument("--snr_gamma", type=float, default=5.0, help="Min-SNR gamma (0=ç¦ç”¨, æ¨è5.0)")
    
    # è®­ç»ƒæ§åˆ¶ (Epoch æ¨¡å¼)
    parser.add_argument("--num_train_epochs", type=int, default=10, help="è®­ç»ƒ Epoch æ•°")
    parser.add_argument("--save_every_n_epochs", type=int, default=1, help="ä¿å­˜é—´éš” (Epoch)")
    
    # å…¼å®¹æ€§ä¿ç•™ (ä¼šè¢«è‡ªåŠ¨è¦†ç›–)
    parser.add_argument("--max_train_steps", type=int, default=None, help="æœ€å¤§è®­ç»ƒæ­¥æ•° (è‡ªåŠ¨è®¡ç®—)")
    parser.add_argument("--save_every_n_steps", type=int, default=None, help="ä¿å­˜é—´éš” (æ­¥æ•°)")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1, help="æ¢¯åº¦ç´¯ç§¯")
    parser.add_argument("--mixed_precision", type=str, default="fp16", choices=["no", "fp16", "bf16"])
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    # é«˜çº§åŠŸèƒ½
    parser.add_argument("--max_grad_norm", type=float, default=1.0, help="æ¢¯åº¦è£å‰ªé˜ˆå€¼")
    parser.add_argument("--gradient_checkpointing", action="store_true", help="å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
    
    # è‡ªåŠ¨ä¼˜åŒ–åŠŸèƒ½
    parser.add_argument("--auto_optimize", action="store_true", default=True, help="å¯ç”¨è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–")
    
    # SPDA (Sequence Parallel DataLoader Adapter) å‚æ•°
    parser.add_argument("--spda_enabled", action="store_true", help="å¯ç”¨SPDAåŠŸèƒ½")
    parser.add_argument("--sequence_parallel", action="store_true", default=True, help="å¯ç”¨åºåˆ—å¹¶è¡Œä¼˜åŒ–")
    parser.add_argument("--ulysses_seq_len", type=int, default=None, help="Ulyssesåºåˆ—é•¿åº¦")
    
    # SDPA (Scaled Dot-Product Attention) å‚æ•°
    parser.add_argument("--attention_backend", type=str, default="sdpa", 
        choices=["sdpa", "flash", "_flash_3"], help="æ³¨æ„åŠ›åç«¯é€‰æ‹©")
    parser.add_argument("--enable_flash_attention", action="store_true", help="å¯ç”¨Flash Attention")
    parser.add_argument("--sdpa_optimize_level", type=str, default="auto",
        choices=["fast", "memory_efficient", "auto"], help="SDPAä¼˜åŒ–çº§åˆ«")
    parser.add_argument("--use_memory_efficient_attention", action="store_true", default=True, help="ä½¿ç”¨å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›")
    parser.add_argument("--attention_dropout", type=float, default=0.0, help="æ³¨æ„åŠ›dropoutç‡")
    parser.add_argument("--force_deterministic", action="store_true", help="å¼ºåˆ¶ç¡®å®šæ€§è®¡ç®—")
    parser.add_argument("--sdpa_min_seq_length", type=int, default=512, help="SDPAæœ€å°åºåˆ—é•¿åº¦é˜ˆå€¼")
    parser.add_argument("--sdpa_batch_size_threshold", type=int, default=4, help="SDPAæ‰¹é‡å¤§å°é˜ˆå€¼")
    
    # Block Swapping (å—äº¤æ¢æŠ€æœ¯) å‚æ•°
    parser.add_argument("--block_swap_enabled", action="store_true", help="å¯ç”¨å—äº¤æ¢æŠ€æœ¯")
    parser.add_argument("--block_swap_block_size", type=int, default=256, help="å—äº¤æ¢å†…å­˜å—å¤§å°")
    parser.add_argument("--block_swap_cpu_buffer_size", type=int, default=1024, help="å—äº¤æ¢CPUç¼“å†²åŒºå¤§å° (MB)")
    parser.add_argument("--block_swap_swap_threshold", type=float, default=0.7, help="å—äº¤æ¢é˜ˆå€¼ (0.1-0.9)")
    parser.add_argument("--block_swap_swap_strategy", type=str, default="lru", choices=["fifo", "lru", "priority"], help="å—äº¤æ¢ç­–ç•¥")
    parser.add_argument("--block_swap_compression_enabled", action="store_true", help="å¯ç”¨å—äº¤æ¢å‹ç¼©")
    parser.add_argument("--block_swap_prefetch_enabled", action="store_true", help="å¯ç”¨å—äº¤æ¢é¢„å–")
    parser.add_argument("--activation_checkpoint_block_size", type=int, default=64, help="æ¿€æ´»æ£€æŸ¥ç‚¹å—å¤§å°")
    parser.add_argument("--memory_monitoring_enabled", action="store_true", help="å¯ç”¨å†…å­˜ç›‘æ§")
    parser.add_argument("--memory_swap_frequency", type=int, default=5, help="å†…å­˜äº¤æ¢é¢‘ç‡")
    parser.add_argument("--memory_pool_strategy", type=str, default="conservative",
        choices=["none", "conservative", "aggressive"], help="å†…å­˜æ± ç­–ç•¥")
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œè¯»å–å¹¶è¦†ç›–é»˜è®¤å€¼
    if args.config:
        import tomli
        with open(args.config, "rb") as f:
            config = tomli.load(f)
            
        # æ‰å¹³åŒ– config å­—å…¸ä»¥ä¾¿æ˜ å°„
        flat_config = {}
        for section in config.values():
            flat_config.update(section)
            
        # æ›´æ–° args (ä»…å½“å‘½ä»¤è¡ŒæœªæŒ‡å®šæ—¶ä½¿ç”¨ config å€¼ï¼Œæˆ–è€…ç›´æ¥è¦†ç›–ï¼Ÿé€šå¸¸å‘½ä»¤è¡Œä¼˜å…ˆçº§æ›´é«˜)
        # è¿™é‡Œæˆ‘ä»¬å®ç°ï¼šConfig è¦†ç›–é»˜è®¤å€¼ï¼Œå‘½ä»¤è¡Œè¦†ç›– Config
        
        # 1. è®¾ç½® Config ä¸­çš„å€¼
        for key, value in flat_config.items():
            # åªæœ‰å½“ args ä¸­å­˜åœ¨è¯¥å±æ€§ä¸”å‘½ä»¤è¡Œæœªæ˜¾å¼æŒ‡å®šï¼ˆè¿™é‡Œæ¯”è¾ƒéš¾åˆ¤æ–­æ˜¯å¦æ˜¾å¼æŒ‡å®šï¼Œ
            # ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾å¦‚æœ config æœ‰å€¼å°±ç”¨ config çš„ï¼Œé™¤é args æ˜¯ Noneï¼‰
            # æ›´ç¨³å¥çš„åšæ³•æ˜¯ï¼šargparse default è®¾ä¸º Noneï¼Œç„¶åæ‰‹åŠ¨å¤„ç† defaults
            if hasattr(args, key):
                setattr(args, key, value)
    
    # å†æ¬¡è§£æå‘½ä»¤è¡Œå‚æ•°ä»¥ç¡®ä¿å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§æœ€é«˜ (éœ€è¦ç¨å¾®é‡æ„ï¼Œæˆ–è€…ç®€å•åœ°åªç”¨ config)
    # ç®€å•å®ç°ï¼šå¦‚æœæä¾›äº† configï¼Œå°±ç”¨ config çš„å€¼è¦†ç›– args çš„é»˜è®¤å€¼
    # ä½†è¿™æ ·å‘½ä»¤è¡Œå‚æ•°å°±æ— æ•ˆäº†ã€‚
    
    # æ›´å¥½çš„å®ç°ï¼š
    # 1. Parse args å¾—åˆ°å‘½ä»¤è¡Œå‚æ•°
    # 2. Load config
    # 3. å¦‚æœå‘½ä»¤è¡Œå‚æ•°æ˜¯é»˜è®¤å€¼ï¼Œä¸” config ä¸­æœ‰å€¼ï¼Œåˆ™ä½¿ç”¨ config çš„å€¼
    # ä½† argparse ä¸å®¹æ˜“åŒºåˆ†"é»˜è®¤å€¼"å’Œ"ç”¨æˆ·è¾“å…¥çš„å€¼"ã€‚
    
    # è¿™ç§æƒ…å†µä¸‹ï¼Œé€šå¸¸å»ºè®®ï¼šå¦‚æœç”¨äº† --configï¼Œå°±ä¸»è¦ä¾èµ– configã€‚
    # æˆ–è€…ï¼Œæˆ‘ä»¬æ‰‹åŠ¨æ£€æŸ¥ sys.argv
    
    # è®©æˆ‘ä»¬é‡‡ç”¨æœ€ç®€å•çš„ç­–ç•¥ï¼šConfig æ–‡ä»¶ä½œä¸º"æ–°çš„é»˜è®¤å€¼"
    if args.config:
        # é‡æ–°è§£æï¼Œè¿™æ¬¡å°† config ä¸­çš„å€¼ä½œä¸º default
        import tomli
        with open(args.config, "rb") as f:
            config = tomli.load(f)
        
        defaults = {}
        for section in config.values():
            defaults.update(section)
            
        parser.set_defaults(**defaults)
        args = parser.parse_args() # å†æ¬¡è§£æï¼Œè¿™æ ·å‘½ä»¤è¡Œå‚æ•°ä¼šè¦†ç›– config (ä½œä¸º defaults)
        
    # éªŒè¯å¿…è¦å‚æ•°
    if not args.dit:
        parser.error("--dit is required (or set in config)")
    
    # dataset_config å¯é€‰ï¼šå¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨ä¸»é…ç½®æ–‡ä»¶
    if not args.dataset_config and args.config:
        args.dataset_config = args.config  # ä½¿ç”¨ä¸»é…ç½®æ–‡ä»¶ä¸­çš„ [dataset] éƒ¨åˆ†
        
    return args


def main():
    args = parse_args()
    
    # ç¡¬ä»¶æ£€æµ‹å’Œè‡ªåŠ¨ä¼˜åŒ–
    logger.info("[DETECT] æ­£åœ¨è¿›è¡Œç¡¬ä»¶æ£€æµ‹...")
    hardware_detector = HardwareDetector()
    hardware_detector.print_detection_summary()
    
    # å¦‚æœå¯ç”¨äº†è‡ªåŠ¨ä¼˜åŒ–ï¼Œåˆ™åº”ç”¨ä¼˜åŒ–é…ç½®
    if args.auto_optimize:
            logger.info("[TARGET] å¯ç”¨è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–...")
            
            # å¦‚æœé…ç½®æ˜¯ç®€åŒ–é…ç½®ï¼Œåº”ç”¨è‡ªåŠ¨ä¼˜åŒ–
            if args.config:
                try:
                    # å°è¯•å¯¼å…¥tomliï¼ˆTOMLè§£æåº“ï¼‰
                    try:
                        import tomli
                        with open(args.config, "rb") as f:
                            config = tomli.load(f)
                    except ImportError:
                        # å¦‚æœæ²¡æœ‰tomliï¼Œä½¿ç”¨tomllibï¼ˆPython 3.11+å†…ç½®ï¼‰
                        import tomllib
                        with open(args.config, "rb") as f:
                            config = tomllib.load(f)
                    
                    # å¦‚æœæ£€æµ‹åˆ°æ˜¯ç®€åŒ–é…ç½®ï¼Œåº”ç”¨è‡ªåŠ¨ä¼˜åŒ–
                    if 'optimization' in config and config['optimization'].get('auto_optimize', False):
                        logger.info("[CONFIG] æ£€æµ‹åˆ°ç®€åŒ–é…ç½®ï¼Œå¼€å§‹è‡ªåŠ¨ä¼˜åŒ–...")
                        
                        # è·å–æ‰‹åŠ¨è¦†ç›–è®¾ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
                        manual_gpu_tier = config['optimization'].get('gpu_tier')
                        manual_gpu_memory = config['optimization'].get('gpu_memory_gb')
                        
                        # åº”ç”¨æ‰‹åŠ¨è¦†ç›–ï¼ˆå¦‚æœæœ‰ï¼‰
                        if manual_gpu_tier:
                            hardware_detector.gpu_info['gpu_tier'] = manual_gpu_tier
                            logger.info(f"[SETUP] æ‰‹åŠ¨è®¾ç½®GPUçº§åˆ«: {manual_gpu_tier}")
                        
                        if manual_gpu_memory:
                            hardware_detector.gpu_info['memory_total'] = manual_gpu_memory
                            logger.info(f"[SETUP] æ‰‹åŠ¨è®¾ç½®GPUæ˜¾å­˜: {manual_gpu_memory}GB")
                        
                        # ä¿å­˜ç”¨æˆ·åœ¨ [advanced] éƒ¨åˆ†è®¾ç½®çš„å€¼
                        user_advanced = config.get('advanced', {})
                        
                        # åº”ç”¨ä¼˜åŒ–é…ç½®
                        optimized_config = hardware_detector.get_optimized_config({})
                        
                        # æ›´æ–°argså¯¹è±¡ï¼ˆä½†ä¿ç•™ç”¨æˆ·æ˜¾å¼è®¾ç½®çš„å€¼ï¼‰
                        for key, value in optimized_config.items():
                            if hasattr(args, key):
                                # å¦‚æœç”¨æˆ·åœ¨ [advanced] ä¸­è®¾ç½®äº†è¯¥å€¼ï¼Œåˆ™ä½¿ç”¨ç”¨æˆ·çš„å€¼
                                if key in user_advanced:
                                    logger.info(f"   {key}: {user_advanced[key]} (ç”¨æˆ·è®¾ç½®)")
                                    setattr(args, key, user_advanced[key])
                                else:
                                    setattr(args, key, value)
                        
                        logger.info("[OK] è‡ªåŠ¨ç¡¬ä»¶ä¼˜åŒ–å®Œæˆ")
                
                except Exception as e:
                    logger.warning(f"[WARN] é…ç½®æ–‡ä»¶è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–: {e}")
                    # ä½¿ç”¨é»˜è®¤ä¼˜åŒ–é…ç½®
                    optimized_config = hardware_detector.get_optimized_config({})
                    for key, value in optimized_config.items():
                        if hasattr(args, key):
                            setattr(args, key, value)
                    logger.info("[OK] ä½¿ç”¨é»˜è®¤ç¡¬ä»¶ä¼˜åŒ–é…ç½®")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆå§‹åŒ– Accelerator
    accelerator = Accelerator(
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        mixed_precision=args.mixed_precision,
    )
    
    # è·å–åˆ†å¸ƒå¼è®­ç»ƒä¿¡æ¯
    world_size = getattr(accelerator, 'num_processes', None)
    rank = getattr(accelerator, 'process_index', None)
    
    # å°†åˆ†å¸ƒå¼ä¿¡æ¯æ·»åŠ åˆ°argsä¸­ï¼Œä¾›SPDAä½¿ç”¨
    args.world_size = world_size
    args.rank = rank
    
    # è®¾ç½®éšæœºç§å­
    if args.seed is not None:
        set_seed(args.seed)
    
    logger.info("="*60)
    logger.info("[START] å¯åŠ¨ AC-RF è®­ç»ƒ")
    logger.info("="*60)
    logger.info(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    logger.info(f"Turbo æ­¥æ•°: {args.turbo_steps}")
    logger.info(f"LoRA rank: {args.network_dim}")
    
    # 1. åŠ è½½æ¨¡å‹
    logger.info("\n[LOAD] åŠ è½½ Transformer...")
    weight_dtype = torch.float32
    if accelerator.mixed_precision == "fp16":
        weight_dtype = torch.float16
    elif accelerator.mixed_precision == "bf16":
        weight_dtype = torch.bfloat16
    
    transformer = load_transformer(
        transformer_path=args.dit,
        device=accelerator.device,
        torch_dtype=weight_dtype,
    )
    transformer.requires_grad_(False)
    transformer.train()  # éœ€è¦è®­ç»ƒæ¨¡å¼ä»¥æ”¯æŒ LoRA
    
    # 1.1 é…ç½®SDPA (Scaled Dot-Product Attention)
    logger.info("\n[INIT] é…ç½® SDPA æ³¨æ„åŠ›åç«¯...")
    logger.info(f"  æ³¨æ„åŠ›åç«¯: {args.attention_backend}")
    logger.info(f"  ä¼˜åŒ–çº§åˆ«: {args.sdpa_optimize_level}")
    logger.info(f"  å†…å­˜é«˜æ•ˆæ³¨æ„åŠ›: {args.use_memory_efficient_attention}")
    logger.info(f"  æ³¨æ„åŠ›dropout: {args.attention_dropout}")
    
    # é…ç½®æ³¨æ„åŠ›åç«¯
    if hasattr(transformer, 'set_attention_backend'):
        try:
            if args.enable_flash_attention:
                # å¦‚æœå¯ç”¨äº†flash attentionï¼Œå°è¯•åˆ‡æ¢åç«¯
                if args.attention_backend == "sdpa":
                    # æ£€æŸ¥ç¡¬ä»¶æ”¯æŒ
                    if torch.cuda.is_available():
                        gpu_name = torch.cuda.get_device_name(0).upper()
                        if "A100" in gpu_name or "H100" in gpu_name:
                            transformer.set_attention_backend("_flash_3")
                            logger.info("  [OK] ç¡¬ä»¶æ£€æµ‹ï¼šå·²å¯ç”¨ Flash Attention 3")
                        elif "RTX" in gpu_name or "4090" in gpu_name or "4080" in gpu_name:
                            transformer.set_attention_backend("flash")
                            logger.info("  [OK] ç¡¬ä»¶æ£€æµ‹ï¼šå·²å¯ç”¨ Flash Attention 2")
                        else:
                            logger.info("  [WARN] ç¡¬ä»¶ä¸æ”¯æŒFlash Attentionï¼Œä½¿ç”¨é»˜è®¤SDPA")
                    else:
                        logger.info("  [WARN] æœªæ£€æµ‹åˆ°CUDAï¼Œä½¿ç”¨é»˜è®¤SDPA")
                else:
                    transformer.set_attention_backend(args.attention_backend)
                    logger.info(f"  [OK] å·²è®¾ç½®æ³¨æ„åŠ›åç«¯ä¸º: {args.attention_backend}")
        except Exception as e:
            logger.warning(f"  [WARN] è®¾ç½®æ³¨æ„åŠ›åç«¯å¤±è´¥: {e}")
            logger.info("  [FALLBACK] ç»§ç»­ä½¿ç”¨é»˜è®¤SDPAå®ç°")
    
    # é…ç½®SDPAç¯å¢ƒå˜é‡
    if args.force_deterministic:
        os.environ['TORCH_DETERMINISTIC'] = '1'
        logger.info("  [LOCK] å·²å¯ç”¨ç¡®å®šæ€§è®¡ç®—")
    
    if args.sdpa_optimize_level == "memory_efficient":
        os.environ['TORCH_CUDA_MEMORY_POOL'] = 'memory_efficient'
        logger.info("  [MEM] å·²å¯ç”¨å†…å­˜ä¼˜åŒ–æ¨¡å¼")
    
    # åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨
    logger.info(f"\n[MEM] åˆå§‹åŒ–å†…å­˜ä¼˜åŒ–å™¨...")
    memory_config = {
        'block_swap_enabled': args.block_swap_enabled,
        'memory_block_size': args.block_swap_block_size,
        'cpu_swap_buffer_size': args.block_swap_cpu_buffer_size,
        'swap_threshold': args.block_swap_swap_threshold,
        'swap_frequency': args.memory_swap_frequency,
        'smart_prefetch': args.block_swap_prefetch_enabled,
        'swap_strategy': args.block_swap_swap_strategy,
        'compressed_swap': args.block_swap_compression_enabled,
        'checkpoint_optimization': 'basic' if args.gradient_checkpointing else 'none',
    }
    memory_optimizer = MemoryOptimizer(memory_config)
    memory_optimizer.start()
    logger.info(f"  [OK] å†…å­˜ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    if args.gradient_checkpointing:
        transformer.enable_gradient_checkpointing()
        logger.info("  [FALLBACK] å·²å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹")
        
    # åº”ç”¨å†…å­˜ä¼˜åŒ–åˆ°transformer
    if hasattr(transformer, 'apply_memory_optimization'):
        transformer.apply_memory_optimization(memory_optimizer)
        logger.info("  [INIT] å·²åº”ç”¨å†…å­˜ä¼˜åŒ–ç­–ç•¥")
        
    # 2. åˆ›å»º LoRA ç½‘ç»œ
    logger.info(f"\n[SETUP] åˆ›å»º LoRA ç½‘ç»œ (rank={args.network_dim})...")
    network = LoRANetwork(
        unet=transformer,
        lora_dim=args.network_dim,
        alpha=args.network_alpha,
        multiplier=1.0,
    )
    network.apply_to(transformer)
    
    # åªè·å– LoRA å±‚çš„å‚æ•°ï¼Œä¸åŒ…æ‹¬åŸå§‹æ¨¡å‹
    trainable_params = []
    for lora_module in network.lora_modules.values():
        trainable_params.extend(lora_module.get_trainable_params())
    
    lora_param_count = sum(p.numel() for p in trainable_params)
    logger.info(f"LoRA å¯è®­ç»ƒå‚æ•°: {lora_param_count:,} ({lora_param_count/1e6:.2f}M)")
    
    # 3. åˆ›å»º AC-RF Trainer
    logger.info(f"\n[INIT] åˆå§‹åŒ– AC-RF Trainer...")
    acrf_trainer = ACRFTrainer(
        num_train_timesteps=1000,
        turbo_steps=args.turbo_steps,
        shift=args.shift,
    )
    acrf_trainer.verify_setup()
    
    # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
    logger.info("\nğŸ“Š åŠ è½½æ•°æ®é›†...")
    dataloader = create_dataloader(args)
    logger.info(f"æ•°æ®é›†å¤§å°: {len(dataloader)} batches")
    
    # 5. è®¡ç®—è®­ç»ƒæ­¥æ•°
    num_update_steps_per_epoch = math.ceil(len(dataloader) / args.gradient_accumulation_steps)
    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
    
    logger.info(f"  Num Epochs = {args.num_train_epochs}")
    logger.info(f"  Num Batches per Epoch = {len(dataloader)}")
    logger.info(f"  Gradient Accumulation = {args.gradient_accumulation_steps}")
    logger.info(f"  Total Optimization Steps = {args.max_train_steps}")
    
    # æ‰“å°æ€»æ­¥æ•°ä¾›å‰ç«¯è§£æï¼ˆå…³é”®ï¼tqdm çš„ \r è¾“å‡ºæ— æ³•è¢« readline æ•è·ï¼‰
    print(f"[TRAINING_INFO] total_steps={args.max_train_steps} total_epochs={args.num_train_epochs}", flush=True)

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    logger.info(f"\n[SETUP] åˆå§‹åŒ–ä¼˜åŒ–å™¨: {args.optimizer_type}")
    
    if args.optimizer_type == "AdamW":
        optimizer = torch.optim.AdamW(
            trainable_params, 
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )
    elif args.optimizer_type == "AdamW8bit":
        try:
            import bitsandbytes as bnb
            optimizer = bnb.optim.AdamW8bit(
                trainable_params, 
                lr=args.learning_rate,
                weight_decay=args.weight_decay
            )
        except ImportError:
            raise ImportError("è¯·å…ˆå®‰è£… bitsandbytes ä»¥ä½¿ç”¨ AdamW8bit ä¼˜åŒ–å™¨")
    elif args.optimizer_type == "Adafactor":
        from transformers.optimization import Adafactor
        logger.info(f"  Adafactor é…ç½®: scale={args.adafactor_scale}, relative={args.adafactor_relative}, warmup={args.adafactor_warmup}")
        optimizer = Adafactor(
            trainable_params,
            lr=args.learning_rate,
            weight_decay=args.weight_decay,
            scale_parameter=args.adafactor_scale,
            relative_step=args.adafactor_relative,
            warmup_init=args.adafactor_warmup
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨ç±»å‹: {args.optimizer_type}")
        
    # 7. åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    from diffusers.optimization import get_scheduler
    logger.info(f"[SCHED] åˆå§‹åŒ–è°ƒåº¦å™¨: {args.lr_scheduler} (warmup={args.lr_warmup_steps}, cycles={args.lr_num_cycles})")
    lr_scheduler = get_scheduler(
        args.lr_scheduler,
        optimizer=optimizer,
        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
        num_cycles=args.lr_num_cycles,
    )
    
    # 7. Accelerator prepare
    transformer, network, optimizer, dataloader, lr_scheduler = accelerator.prepare(
        transformer, network, optimizer, dataloader, lr_scheduler
    )
    
    # 8. è®­ç»ƒå¾ªç¯
    logger.info("\n" + "="*60)
    logger.info("[TARGET] å¼€å§‹è®­ç»ƒ")
    logger.info("="*60)
    
    global_step = 0
    progress_bar = tqdm(total=args.max_train_steps, desc="Training")
    
    # EMA å¹³æ»‘ lossï¼ˆç”¨äºæ˜¾ç¤ºè¶‹åŠ¿ï¼Œä¸å½±å“è®­ç»ƒï¼‰
    ema_loss = None
    ema_decay = 0.99  # å¹³æ»‘ç³»æ•°
    
    for epoch in range(args.num_train_epochs):
        logger.info(f"\nEpoch {epoch + 1}/{args.num_train_epochs}")
        transformer.train()
        
        for step, batch in enumerate(dataloader):
            with accelerator.accumulate(network):
                # è·å–æ•°æ®
                latents = batch['latents'].to(accelerator.device, dtype=weight_dtype)
                vl_embed = batch['vl_embed']  # List of tensors
                
                # ç¡®ä¿ vl_embed ä¸­çš„æ‰€æœ‰å¼ é‡éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if isinstance(vl_embed, list):
                    vl_embed = [tensor.to(accelerator.device, dtype=weight_dtype) for tensor in vl_embed]
                else:
                    vl_embed = vl_embed.to(accelerator.device, dtype=weight_dtype)
                
                # ç”Ÿæˆå™ªå£°
                noise = torch.randn_like(latents)
                
                # AC-RF é‡‡æ ·
                noisy_latents, timesteps, target_velocity = acrf_trainer.sample_batch(
                    latents, noise, jitter_scale=args.jitter_scale
                )
                
                # å‡†å¤‡æ¨¡å‹è¾“å…¥
                # Z-Image expects List[Tensor(C, 1, H, W)]
                model_input = noisy_latents.unsqueeze(2)  # (B, C, 1, H, W)
                model_input_list = list(model_input.unbind(dim=0))
                
                # Timestep normalization (Z-Image uses (1000-t)/1000)
                timesteps_normalized = (1000 - timesteps) / 1000.0
                timesteps_normalized = timesteps_normalized.to(dtype=weight_dtype)
                
                # å‰å‘ä¼ æ’­
                model_pred_list = transformer(
                    x=model_input_list,
                    t=timesteps_normalized,
                    cap_feats=vl_embed,
                )[0]
                
                # Stack outputs
                model_pred = torch.stack(model_pred_list, dim=0)
                model_pred = model_pred.squeeze(2)  # (B, C, H, W)
                
                # Z-Image è¾“å‡ºæ˜¯è´Ÿçš„
                model_pred = -model_pred
                
                # è®¡ç®—æŸå¤±
                loss = acrf_trainer.compute_loss(
                    model_output=model_pred,
                    target_velocity=target_velocity,
                    latents_noisy=noisy_latents,
                    timesteps=timesteps,
                    target_x0=latents,  # åŸå§‹å¹²å‡€çš„ latents
                    lambda_fft=args.lambda_fft,
                    lambda_cosine=args.lambda_cosine,
                    snr_gamma=args.snr_gamma,  # Min-SNR gamma (0=ç¦ç”¨)
                )
                
                # åå‘ä¼ æ’­
                accelerator.backward(loss)
            
            # åªåœ¨æ¢¯åº¦ç´¯ç§¯å®Œæˆåæ‰§è¡Œä¼˜åŒ–æ­¥éª¤
            if accelerator.sync_gradients:
                # æ¢¯åº¦è£å‰ª
                accelerator.clip_grad_norm_(trainable_params, args.max_grad_norm)
                
                # ä¼˜åŒ–å™¨æ­¥è¿›
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()
                
                # æ›´æ–°è¿›åº¦
                progress_bar.update(1)
                global_step += 1
                
                # æ›´æ–° EMA lossï¼ˆå¹³æ»‘æ˜¾ç¤ºï¼Œå‡å°‘è·³åŠ¨çš„è§†è§‰å¹²æ‰°ï¼‰
                current_loss = loss.item()
                if ema_loss is None:
                    ema_loss = current_loss
                else:
                    ema_loss = ema_decay * ema_loss + (1 - ema_decay) * current_loss
                
                # æ˜¾ç¤ºï¼šå½“å‰ lossã€EMA å’Œå­¦ä¹ ç‡
                current_lr = lr_scheduler.get_last_lr()[0]
                progress_bar.set_postfix({
                    "loss": f"{current_loss:.4f}",
                    "ema": f"{ema_loss:.4f}",
                    "lr": f"{current_lr:.2e}"
                })
                
                # å®šæœŸæ‰“å°è¿›åº¦ä¾›å‰ç«¯è§£æï¼ˆæ¯10æ­¥æˆ–æ¯æ­¥éƒ½æ‰“å°ï¼‰
                if global_step % 1 == 0:  # æ¯æ­¥éƒ½æ‰“å°
                    print(f"[STEP] {global_step}/{args.max_train_steps} epoch={epoch+1}/{args.num_train_epochs} loss={current_loss:.4f} ema_loss={ema_loss:.4f} lr={current_lr:.2e}", flush=True)
                
            # æ‰§è¡Œå†…å­˜ä¼˜åŒ– (æ¸…ç†ç¼“å­˜ç­‰)
            memory_optimizer.optimize_training_step()
                
        # Epoch ç»“æŸï¼Œä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % args.save_every_n_epochs == 0:
            save_path = Path(args.output_dir) / f"acrf_lora_epoch{epoch+1}.safetensors"
            network.save_weights(save_path, dtype=weight_dtype)
            logger.info(f"\n[SAVE] ä¿å­˜æ£€æŸ¥ç‚¹ (Epoch {epoch+1}): {save_path}")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = Path(args.output_dir) / "acrf_lora_final.safetensors"
    network.save_weights(final_path, dtype=weight_dtype)
    
    # åœæ­¢å†…å­˜ä¼˜åŒ–å™¨
    memory_optimizer.stop()
    
    logger.info("\n" + "="*60)
    logger.info(f"[OK] è®­ç»ƒå®Œæˆï¼")
    logger.info(f"æœ€ç»ˆæ¨¡å‹: {final_path}")
    logger.info("="*60)


if __name__ == "__main__":
    main()
