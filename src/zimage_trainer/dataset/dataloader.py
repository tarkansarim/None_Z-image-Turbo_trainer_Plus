# -*- coding: utf-8 -*-
"""
Dataset and DataLoader for Z-Image training.

Standalone implementation - no musubi-tuner dependency.
Includes SPDA (Sequence Parallel DataLoader Adapter) support.
"""

import os
import glob
import logging
import math
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

import torch
from torch.utils.data import Dataset, DataLoader
from safetensors.torch import load_file

try:
    import toml
except ImportError:
    try:
        import tomli as toml
    except ImportError:
        toml = None

logger = logging.getLogger(__name__)


class SPDALoaderAdapter:
    """
    Sequence Parallel DataLoader Adapter (SPDA)
    ä¸ºåºåˆ—å¹¶è¡Œè®­ç»ƒä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨é€‚é…å™¨
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. åºåˆ—å¹¶è¡Œæ•°æ®åŠ è½½
    2. åŠ¨æ€æ‰¹æ¬¡å¤§å°è°ƒæ•´
    3. å†…å­˜æ•ˆç‡ä¼˜åŒ–
    4. æ”¯æŒå¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
    """
    
    def __init__(
        self,
        original_dataloader: DataLoader,
        sequence_parallel: bool = True,
        world_size: Optional[int] = None,
        rank: Optional[int] = None,
        gradient_accumulation_steps: int = 1,
        ulysses_seq_len: Optional[int] = None,
    ):
        self.original_dataloader = original_dataloader
        self.sequence_parallel = sequence_parallel
        self.world_size = world_size or torch.cuda.device_count() if torch.cuda.is_available() else 1
        self.rank = rank or 0
        self.gradient_accumulation_steps = gradient_accumulation_steps
        self.ulysses_seq_len = ulysses_seq_len
        self.is_distributed = self.world_size > 1
        
        # åºåˆ—å¹¶è¡Œç›¸å…³é…ç½®
        self.enable_ulysses = ulysses_seq_len is not None
        if self.enable_ulysses:
            logger.info(f"å¯ç”¨Ulyssesåºåˆ—å¹¶è¡Œï¼Œåºåˆ—é•¿åº¦: {ulysses_seq_len}")
        
        if self.sequence_parallel:
            logger.info(f"å¯ç”¨SPDAåºåˆ—å¹¶è¡Œ - World Size: {self.world_size}, Rank: {self.rank}")
        
        # ç¼“å­˜æœºåˆ¶
        self._cache = {}
        self._cache_size = 10  # ç¼“å­˜æ‰¹æ¬¡æ•°é‡
        
    def __iter__(self):
        """è¿”å›é€‚é…å™¨çš„è¿­ä»£å™¨"""
        self.dataloader_iter = iter(self.original_dataloader)
        self._step_counter = 0
        self._skipped_batches = 0
        return self
    
    def __next__(self):
        """è·å–ä¸‹ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®"""
        try:
            # è·³è¿‡æŸäº›æ‰¹æ¬¡ä»¥ä¿æŒåˆ†å¸ƒå¼è®­ç»ƒåŒæ­¥
            if self.is_distributed and self._skipped_batches < self.rank:
                next(self.dataloader_iter)
                self._skipped_batches += 1
                return self.__next__()  # é€’å½’è°ƒç”¨è·å–ä¸‹ä¸€ä¸ªæœ‰æ•ˆæ‰¹æ¬¡
            
            batch = next(self.dataloader_iter)
            self._step_counter += 1
            
            # ç¡®ä¿batchæ˜¯å­—å…¸ç±»å‹
            if not isinstance(batch, dict):
                raise TypeError(f"Expected batch to be dict, got {type(batch)}")
            
            return self._apply_sequence_parallel_optimization(batch)
            
        except StopIteration:
            raise StopIteration
        
    def __len__(self):
        """è¿”å›æ•°æ®åŠ è½½å™¨çš„é•¿åº¦"""
        return len(self.original_dataloader)
            
    def _apply_sequence_parallel_optimization(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """åº”ç”¨åºåˆ—å¹¶è¡Œä¼˜åŒ–åˆ°æ‰¹æ¬¡"""
        optimized_batch = batch.copy()
        
        # 1. åºåˆ—é•¿åº¦ä¼˜åŒ–
        if self.enable_ulysses:
            optimized_batch = self._apply_ulysses_optimization(optimized_batch)
            
        # 2. å†…å­˜ä¼˜åŒ–
        if self.sequence_parallel:
            optimized_batch = self._apply_memory_optimization(optimized_batch)
            
        # 3. æ‰¹æ¬¡å¤§å°åŠ¨æ€è°ƒæ•´
        optimized_batch = self._apply_dynamic_batch_sizing(optimized_batch)
        
        return optimized_batch
        
    def _apply_ulysses_optimization(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """åº”ç”¨Ulyssesåºåˆ—å¹¶è¡Œä¼˜åŒ–"""
        if 'vl_embed' in batch:
            # è·å–åºåˆ—é•¿åº¦
            seq_lens = []
            for embed in batch['vl_embed']:
                if hasattr(embed, 'shape'):
                    seq_lens.append(embed.shape[0])
                else:
                    seq_lens.append(len(embed))
            
            # åºåˆ—é•¿åº¦å¯¹é½åˆ°ulysses_seq_lençš„å€æ•°
            target_len = self.ulysses_seq_len
            if target_len:
                for i, embed in enumerate(batch['vl_embed']):
                    if hasattr(embed, 'shape'):
                        current_len = embed.shape[0]
                        if current_len > target_len:
                            # æˆªæ–­åˆ°ç›®æ ‡é•¿åº¦
                            batch['vl_embed'][i] = embed[:target_len]
                        elif current_len < target_len:
                            # å¡«å……åˆ°ç›®æ ‡é•¿åº¦
                            pad_len = target_len - current_len
                            if len(embed.shape) == 2:
                                pad_tensor = torch.zeros(pad_len, embed.shape[1], device=embed.device)
                                batch['vl_embed'][i] = torch.cat([embed, pad_tensor], dim=0)
                            else:
                                pad_tensor = torch.full((pad_len,), -1, device=embed.device)
                                batch['vl_embed'][i] = torch.cat([embed, pad_tensor], dim=0)
                                
        return batch
        
    def _apply_memory_optimization(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """åº”ç”¨å†…å­˜ä¼˜åŒ–"""
        # åºåˆ—å¹¶è¡Œåˆ†å‰²
        if self.world_size > 1:
            for key, value in batch.items():
                if torch.is_tensor(value) and value.dim() > 1:
                    # æŒ‰åºåˆ—ç»´åº¦åˆ†å‰²
                    if key == 'latents' and value.dim() == 4:
                        # å¯¹äºlatentsï¼ŒæŒ‰heightåˆ†å‰²
                        h = value.shape[2]
                        split_size = h // self.world_size
                        if split_size > 0:
                            splits = torch.split(value, split_size, dim=2)
                            batch[key] = splits[self.rank]
                    elif key == 'vl_embed':
                        # å¯¹äºvl_embedï¼Œå¤„ç†listæ ¼å¼
                        if isinstance(value, list):
                            # å¯¹listä¸­çš„æ¯ä¸ªtensorè¿›è¡Œåˆ†å‰²
                            for i, embed in enumerate(value):
                                if torch.is_tensor(embed) and embed.dim() > 1:
                                    seq_len = embed.shape[0]
                                    split_size = seq_len // self.world_size
                                    if split_size > 0:
                                        splits = torch.split(embed, split_size, dim=0)
                                        batch[key][i] = splits[self.rank]
                                        
        return batch
        
    def _apply_dynamic_batch_sizing(self, batch: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """åº”ç”¨åŠ¨æ€æ‰¹æ¬¡å¤§å°è°ƒæ•´"""
        # æ ¹æ®åºåˆ—é•¿åº¦åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if 'vl_embed' in batch and isinstance(batch['vl_embed'], list):
            seq_lens = []
            for embed in batch['vl_embed']:
                if hasattr(embed, 'shape'):
                    seq_lens.append(embed.shape[0])
                else:
                    seq_lens.append(len(embed))
            
            max_seq_len = max(seq_lens)
            base_batch_size = batch['latents'].shape[0] if 'latents' in batch else 1
            
            # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
            if max_seq_len > 512:
                # é•¿åºåˆ—æ—¶å‡å°æ‰¹æ¬¡å¤§å°
                adjustment_factor = min(0.5, 512.0 / max_seq_len)
                new_batch_size = max(1, int(base_batch_size * adjustment_factor))
                
                if new_batch_size < base_batch_size:
                    # æˆªæ–­æ‰¹æ¬¡
                    for key, value in batch.items():
                        if torch.is_tensor(value) and value.shape[0] > new_batch_size:
                            batch[key] = value[:new_batch_size]
                    logger.debug(f"åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°: {base_batch_size} -> {new_batch_size} (max_seq_len: {max_seq_len})")
                    
        return batch
        
    def get_sequence_parallel_info(self) -> Dict[str, Union[bool, int]]:
        """è·å–åºåˆ—å¹¶è¡Œä¿¡æ¯"""
        return {
            'sequence_parallel_enabled': self.sequence_parallel,
            'world_size': self.world_size,
            'rank': self.rank,
            'ulysses_enabled': self.enable_ulysses,
            'ulysses_seq_len': self.ulysses_seq_len,
            'gradient_accumulation_steps': self.gradient_accumulation_steps,
        }


class ZImageLatentDataset(Dataset):
    """
    Dataset for loading pre-cached latents and text embeddings.
    Supports multiple datasets and per-dataset resolution filtering.
    """
    
    LATENT_ARCH = "zi"
    TE_SUFFIX = "_zi_te.safetensors"
    
    def __init__(
        self,
        datasets: List[Dict],
        shuffle: bool = True,
    ):
        super().__init__()
        
        self.datasets = datasets
        self.shuffle = shuffle
        
        self.cache_files = []
        self.resolutions = []
        # === CUSTOM: Track which dataset each sample came from ===
        self.dataset_indices = []
        
        for ds_idx, ds_config in enumerate(datasets):
            cache_dir = Path(ds_config['cache_directory'])
            repeats = ds_config.get('num_repeats', 1)
            resolution_limit = ds_config.get('resolution_limit', None)
            
            logger.info(f"Loading dataset from: {cache_dir} (repeats={repeats}, limit={resolution_limit})")
            
            files, res_list = self._load_dataset(cache_dir, resolution_limit)
            
            # Apply repeats
            if repeats > 1:
                files = files * repeats
                res_list = res_list * repeats
            
            self.cache_files.extend(files)
            self.resolutions.extend(res_list)
            # === CUSTOM: Track dataset_idx for each file ===
            self.dataset_indices.extend([ds_idx] * len(files))
            
        if len(self.cache_files) == 0:
            raise ValueError(
                "No valid cache files found in any dataset. "
                "Check the warnings above - common causes:\n"
                "  1. resolution_limit is too low (all files filtered out)\n"
                "  2. Cache files don't exist (need to run latent caching first)\n"
                "  3. Text encoder cache files are missing"
            )
            
        logger.info(f"Total samples: {len(self.cache_files)}")
    
    def _load_dataset(self, cache_dir: Path, resolution_limit: Optional[int]) -> Tuple[List[Tuple[Path, Path]], List[Tuple[int, int]]]:
        """Load files from a single directory and filter by resolution"""
        files = []
        resolutions = []
        
        # Find all latent files
        pattern = f"*_{self.LATENT_ARCH}.safetensors"
        latent_files = list(cache_dir.glob(pattern))
        
        if not latent_files:
            logger.warning(f"  No latent cache files (*_{self.LATENT_ARCH}.safetensors) found in {cache_dir}")
            return files, resolutions
        
        filtered_by_resolution = 0
        missing_te_cache = 0
        
        for latent_path in latent_files:
            # Parse resolution
            res = self._parse_resolution(latent_path.stem)
            
            # Filter by resolution limit
            if resolution_limit:
                h, w = res
                if max(h, w) > resolution_limit:
                    filtered_by_resolution += 1
                    continue
            
            # Find text encoder cache
            te_path = self._find_te_path(latent_path, cache_dir)
            
            if te_path and te_path.exists():
                files.append((latent_path, te_path))
                resolutions.append(res)
            else:
                missing_te_cache += 1
        
        # Log helpful info if files were filtered
        if filtered_by_resolution > 0:
            logger.warning(f"  {filtered_by_resolution}/{len(latent_files)} files filtered by resolution_limit={resolution_limit}")
        if missing_te_cache > 0:
            logger.warning(f"  {missing_te_cache} files missing text encoder cache")
        if len(files) > 0:
            logger.info(f"  Loaded {len(files)} samples from {cache_dir.name}")
            
        return files, resolutions

    def _parse_resolution(self, name: str) -> Tuple[int, int]:
        """Parse resolution from filename (e.g., image_1024x1024_zi)"""
        parts = name.split('_')
        res = (1024, 1024) # Default
        for part in parts:
            if 'x' in part and part.replace('x', '').isdigit():
                try:
                    w, h = map(int, part.split('x'))
                    res = (h, w) # (H, W)
                    break
                except:
                    pass
        return res

    def _find_te_path(self, latent_path: Path, cache_dir: Path) -> Optional[Path]:
        """Construct text encoder cache path"""
        name = latent_path.stem
        parts = name.rsplit('_', 2)
        if len(parts) >= 3:
            base_name = parts[0]
        else:
            base_name = name.rsplit('_', 1)[0]
        
        return cache_dir / f"{base_name}{self.TE_SUFFIX}"
    
    def __len__(self) -> int:
        return len(self.cache_files)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        latent_path, te_path = self.cache_files[idx]
        
        # Load latent
        latent_data = load_file(str(latent_path))
        latent_key = next((k for k in latent_data.keys() if k.startswith('latents_')), None)
        if latent_key is None:
            raise ValueError(f"No latent key found in {latent_path}")
        latents = latent_data[latent_key]
        
        # ç¡®ä¿latentå°ºå¯¸èƒ½è¢«patch_size=2æ•´é™¤ï¼ˆä¸ºTransformerå‡†å¤‡ï¼‰
        C, H, W = latents.shape
        patch_size = 2
        
        # è®¡ç®—éœ€è¦å¡«å……çš„å°ºå¯¸
        H_padded = ((H + patch_size - 1) // patch_size) * patch_size
        W_padded = ((W + patch_size - 1) // patch_size) * patch_size
        
        if H != H_padded or W != W_padded:
            # å¡«å……latentåˆ°åˆé€‚çš„å°ºå¯¸ (left, right, top, bottom)
            latents = torch.nn.functional.pad(
                latents, 
                (0, W_padded - W, 0, H_padded - H),  # (left, right, top, bottom)
                mode='reflect'
            )
        
        # Load text encoder output
        te_data = load_file(str(te_path))
        vl_embed_key = next((k for k in te_data.keys() if 'vl_embed' in k), None)
        if vl_embed_key is None:
            raise ValueError(f"No vl_embed key found in {te_path}")
        vl_embed = te_data[vl_embed_key]
        
        return {
            'latents': latents,
            'vl_embed': vl_embed,
            # === CUSTOM: Include dataset_idx for per-dataset loss weights ===
            'dataset_idx': self.dataset_indices[idx],
        }


class BucketBatchSampler(torch.utils.data.Sampler):
    """
    æ”¯æŒåˆ†æ¡¶çš„ Batch Samplerã€‚
    å°†å…·æœ‰ç›¸åŒåˆ†è¾¨ç‡çš„æ ·æœ¬ç»„åˆåœ¨ä¸€èµ·ã€‚
    """
    def __init__(self, dataset, batch_size, drop_last=False, shuffle=True):
        self.dataset = dataset
        self.batch_size = batch_size
        self.drop_last = drop_last
        self.shuffle = shuffle
        
        # æŒ‰åˆ†è¾¨ç‡åˆ†ç»„ç´¢å¼•
        self.buckets = {} # (h, w) -> [indices]
        for idx, res in enumerate(dataset.resolutions):
            if res not in self.buckets:
                self.buckets[res] = []
            self.buckets[res].append(idx)
            
    def __iter__(self):
        batches = []
        for res, indices in self.buckets.items():
            if self.shuffle:
                # æ‰“ä¹±æ¡¶å†…ç´¢å¼•
                indices = torch.tensor(indices)[torch.randperm(len(indices))].tolist()
            
            # ç”Ÿæˆ batch
            for i in range(0, len(indices), self.batch_size):
                batch = indices[i:i + self.batch_size]
                if len(batch) == self.batch_size or not self.drop_last:
                    batches.append(batch)
        
        if self.shuffle:
            # æ‰“ä¹± batch é¡ºåº
            import random
            random.shuffle(batches)
            
        for batch in batches:
            yield batch

    def __len__(self):
        count = 0
        for indices in self.buckets.values():
            if self.drop_last:
                count += len(indices) // self.batch_size
            else:
                count += (len(indices) + self.batch_size - 1) // self.batch_size
        return count


def collate_fn(batch: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
    """
    è‡ªå®šä¹‰ collate å‡½æ•°ã€‚æ”¯æŒä¸åŒåˆ†è¾¨ç‡çš„ latentï¼ˆè‡ªåŠ¨ paddingï¼‰ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ latents å…·æœ‰ç›¸åŒå½¢çŠ¶
    shapes = [item['latents'].shape for item in batch]
    all_same = all(s == shapes[0] for s in shapes)
    
    if all_same:
        # æ‰€æœ‰å½¢çŠ¶ç›¸åŒï¼Œç›´æ¥ stack
        latents = torch.stack([item['latents'] for item in batch])
    else:
        # å½¢çŠ¶ä¸åŒï¼Œéœ€è¦ padding åˆ°æœ€å¤§å°ºå¯¸
        max_h = max(s[1] for s in shapes)
        max_w = max(s[2] for s in shapes)
        
        # ç¡®ä¿å°ºå¯¸èƒ½è¢« patch_size=2 æ•´é™¤
        patch_size = 2
        max_h = ((max_h + patch_size - 1) // patch_size) * patch_size
        max_w = ((max_w + patch_size - 1) // patch_size) * patch_size
        
        padded_latents = []
        for item in batch:
            lat = item['latents']
            c, h, w = lat.shape
            if h < max_h or w < max_w:
                # Pad to max size (right and bottom padding)
                lat = torch.nn.functional.pad(
                    lat,
                    (0, max_w - w, 0, max_h - h),
                    mode='constant',
                    value=0
                )
            padded_latents.append(lat)
        
        latents = torch.stack(padded_latents)
        logger.debug(f"Padded latents from {shapes} to {latents.shape}")
    
    vl_embeds = [item['vl_embed'] for item in batch]  # ä¿æŒ list å½¢å¼
    
    # === CUSTOM: Batch dataset_idx for per-dataset loss weights ===
    dataset_indices = torch.tensor([item['dataset_idx'] for item in batch], dtype=torch.long)
    
    return {
        'latents': latents,
        'vl_embed': vl_embeds,
        'dataset_idx': dataset_indices,
    }


def create_dataloader(args) -> Union[DataLoader, SPDALoaderAdapter]:
    """
    ä»é…ç½®åˆ›å»º DataLoaderï¼Œæ”¯æŒSPDA (Sequence Parallel DataLoader Adapter)ã€‚
    
    Args:
        args: è®­ç»ƒå‚æ•°ï¼ŒåŒ…å«dataset_configå’Œå…¶ä»–ç›¸å…³é…ç½®
        
    Returns:
        DataLoader or SPDALoaderAdapter: åŸå§‹æ•°æ®åŠ è½½å™¨æˆ–SPDAä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨
    """
    # è¯»å– dataset é…ç½®
    if hasattr(args, 'dataset_config') and args.dataset_config:
        config = _read_dataset_config(args.dataset_config)
    else:
        config = {}
    
    # è·å–å‚æ•°
    datasets = config.get('datasets', [])
    
    # å…¼å®¹æ—§é…ç½® (å¦‚æœ config ä¸­æ²¡æœ‰ datasetsï¼Œå°è¯•ä» args æˆ–æ—§ config è¯»å–)
    if not datasets:
        cache_dir = config.get('cache_directory', getattr(args, 'cache_directory', None))
        if cache_dir:
            datasets = [{
                'cache_directory': cache_dir,
                'num_repeats': config.get('num_repeats', getattr(args, 'num_repeats', 1)),
                'resolution_limit': config.get('resolution_limit', None) # å…¼å®¹æ—§çš„ global limit
            }]
    
    if not datasets:
        raise ValueError("No datasets configured. Please check dataset_config.toml or arguments.")
    
    batch_size = config.get('batch_size', getattr(args, 'batch_size', 4))
    num_workers = config.get('num_workers', getattr(args, 'num_workers', 4))
    
    # åˆ†æ¡¶è®¾ç½®ï¼š--disable_bucket ä¼˜å…ˆçº§æœ€é«˜
    if getattr(args, 'disable_bucket', False):
        enable_bucket = False
    else:
        enable_bucket = config.get('enable_bucket', getattr(args, 'enable_bucket', True))
    
    # SPDAç›¸å…³å‚æ•°
    spda_enabled = config.get('spda_enabled', getattr(args, 'spda_enabled', False))
    sequence_parallel = config.get('sequence_parallel', getattr(args, 'sequence_parallel', True))
    ulysses_seq_len = config.get('ulysses_seq_len', getattr(args, 'ulysses_seq_len', None))
    
    # åˆ†å¸ƒå¼è®­ç»ƒå‚æ•°
    world_size = getattr(args, 'world_size', None)
    rank = getattr(args, 'rank', None)
    gradient_accumulation_steps = getattr(args, 'gradient_accumulation_steps', 1)
    
    # åˆ›å»º dataset
    dataset = ZImageLatentDataset(
        datasets=datasets,
    )
    
    if enable_bucket:
        logger.info("ğŸŒŠ å¯ç”¨åˆ†æ¡¶ (BucketBatchSampler)")
        batch_sampler = BucketBatchSampler(
            dataset, 
            batch_size=batch_size,
            drop_last=True,
            shuffle=True
        )
        dataloader = DataLoader(
            dataset,
            batch_sampler=batch_sampler,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
        )
    else:
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=collate_fn,
            pin_memory=True,
            drop_last=True,
        )
    
    # åº”ç”¨SPDAä¼˜åŒ–
    if spda_enabled:
        logger.info("ğŸš€ å¯ç”¨SPDA (Sequence Parallel DataLoader Adapter)")
        
        spda_adapter = SPDALoaderAdapter(
            original_dataloader=dataloader,
            sequence_parallel=sequence_parallel,
            world_size=world_size,
            rank=rank,
            gradient_accumulation_steps=gradient_accumulation_steps,
            ulysses_seq_len=ulysses_seq_len,
        )
        
        # æ‰“å°SPDAé…ç½®ä¿¡æ¯
        spda_info = spda_adapter.get_sequence_parallel_info()
        logger.info(f"SPDAé…ç½®: {spda_info}")
        
        return spda_adapter
    else:
        logger.info("ğŸ“¦ ä½¿ç”¨æ ‡å‡†DataLoader")
        return dataloader


def _read_dataset_config(config_path: str) -> dict:
    """
    è¯»å– dataset é…ç½®æ–‡ä»¶ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
    
    1. åˆå¹¶æ ¼å¼ (æ–°): [dataset] + [[dataset.sources]] åœ¨ä¸»é…ç½®ä¸­
    2. ç‹¬ç«‹æ ¼å¼ (æ—§): [general] + [[datasets]] åœ¨å•ç‹¬æ–‡ä»¶ä¸­
    3. æ—§æ ¼å¼: [dataset] å—
    """
    if toml is None:
        return {}
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = toml.load(f)
    
    # 1. åˆå¹¶æ ¼å¼ (æ–°): [dataset] + [[dataset.sources]] 
    #    ä¸»é…ç½®æ–‡ä»¶ä¸­çš„ dataset å—
    if 'dataset' in config:
        dataset_config = config['dataset'].copy()
        # å°† sources é‡å‘½åä¸º datasets (å…¼å®¹ create_dataloader)
        if 'sources' in dataset_config:
            dataset_config['datasets'] = dataset_config.pop('sources')
        return dataset_config
    
    # 2. ç‹¬ç«‹æ ¼å¼: [general] + [[datasets]]
    if 'datasets' in config:
        # å¦‚æœæœ‰ [general] å—ï¼Œåˆå¹¶åˆ°é¡¶å±‚
        if 'general' in config:
            config.update(config['general'])
        return config
    
    # 3. æ ¹çº§åˆ«é…ç½® (å…¼å®¹æ—§ç‰ˆ)
    return config