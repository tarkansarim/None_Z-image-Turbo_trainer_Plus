"""
ç¡¬ä»¶æ£€æµ‹å’Œè‡ªåŠ¨ä¼˜åŒ–é…ç½®æ¨¡å—
è‡ªåŠ¨æ£€æµ‹ GPU ç±»å‹å¹¶ä¼˜åŒ–è®­ç»ƒå‚æ•°

æ”¯æŒæ£€æµ‹:
- GPU ç±»å‹å’Œæ˜¾å­˜
- xformers å¯ç”¨æ€§
- Flash Attention æ”¯æŒ
- SDPA æ”¯æŒ
"""
import torch
import psutil
import logging
from typing import Dict, Any, Tuple, Optional

logger = logging.getLogger(__name__)

# xformers æ£€æµ‹
XFORMERS_AVAILABLE = False
XFORMERS_VERSION = None

try:
    import xformers
    import xformers.ops as xops
    XFORMERS_AVAILABLE = True
    XFORMERS_VERSION = getattr(xformers, "__version__", "unknown")
except ImportError:
    pass
except Exception:
    pass

class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å’Œè‡ªåŠ¨ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.gpu_info = self.detect_gpu()
        self.cpu_info = self.detect_cpu()
        self.memory_info = self.detect_memory()
        self.xformers_info = self.detect_xformers()
        self.attention_info = self.detect_attention_backends()
        
    def detect_gpu(self) -> Dict[str, Any]:
        """æ£€æµ‹ GPU ä¿¡æ¯"""
        gpu_info = {
            "available": torch.cuda.is_available(),
            "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "device_name": "CPU",
            "compute_capability": None,
            "memory_total": 0,
            "memory_free": 0,
            "gpu_tier": "unknown"
        }
        
        if not torch.cuda.is_available():
            logger.info("[INFO] No CUDA GPU detected, will use CPU training")
            return gpu_info
            
        # è·å–ä¸» GPU ä¿¡æ¯
        gpu_info["device_name"] = torch.cuda.get_device_name(0)
        gpu_info["memory_total"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
        gpu_info["compute_capability"] = torch.cuda.get_device_properties(0).major
        gpu_info["compute_capability"] = (gpu_info["compute_capability"], torch.cuda.get_device_properties(0).minor)
        
        # è®¡ç®—å¯ç”¨å†…å­˜
        gpu_info["memory_free"] = gpu_info["memory_total"] - torch.cuda.memory_allocated() / (1024**3)
        
        # GPU åˆ†çº§
        gpu_info["gpu_tier"] = self._classify_gpu_tier(gpu_info["device_name"], gpu_info["memory_total"])
        
        logger.info(f"ğŸ–¥ï¸ æ£€æµ‹åˆ° GPU: {gpu_info['device_name']}")
        logger.info(f"[VRAM] GPU Memory: {gpu_info['memory_total']:.1f}GB (Free: {gpu_info['memory_free']:.1f}GB)")
        logger.info(f"[TIER] GPU Tier: {gpu_info['gpu_tier']}")
        
        return gpu_info
    
    def detect_cpu(self) -> Dict[str, Any]:
        """æ£€æµ‹ CPU ä¿¡æ¯"""
        cpu_count = psutil.cpu_count(logical=True)
        cpu_freq = psutil.cpu_freq()
        
        return {
            "count": cpu_count,
            "frequency": cpu_freq.max if cpu_freq else None,
            "arch": "x86_64"
        }
    
    def detect_memory(self) -> Dict[str, Any]:
        """æ£€æµ‹ç³»ç»Ÿå†…å­˜ä¿¡æ¯"""
        memory = psutil.virtual_memory()
        
        return {
            "total": memory.total / (1024**3),  # GB
            "available": memory.available / (1024**3),  # GB
            "percent": memory.percent
        }
    
    def detect_xformers(self) -> Dict[str, Any]:
        """æ£€æµ‹ xformers å¯ç”¨æ€§å’ŒåŠŸèƒ½"""
        info = {
            "available": XFORMERS_AVAILABLE,
            "version": XFORMERS_VERSION,
            "memory_efficient_attention": False,
            "flash_attention": False,
            "cutlass": False,
        }
        
        if not XFORMERS_AVAILABLE:
            logger.info("[WARN] xformers not installed")
            return info
        
        try:
            info["memory_efficient_attention"] = hasattr(xops, "memory_efficient_attention")
            
            if hasattr(xops, "MemoryEfficientAttentionFlashAttentionOp"):
                info["flash_attention"] = True
            
            if hasattr(xops, "MemoryEfficientAttentionCutlassOp"):
                info["cutlass"] = True
            
            logger.info(f"[OK] xformers {XFORMERS_VERSION} available")
            if info["flash_attention"]:
                logger.info("   [OK] Flash Attention supported")
            if info["cutlass"]:
                logger.info("   [OK] CUTLASS supported")
                
        except Exception as e:
            logger.warning(f"xformers åŠŸèƒ½æ£€æµ‹å¤±è´¥: {e}")
        
        return info
    
    def detect_attention_backends(self) -> Dict[str, Any]:
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„æ³¨æ„åŠ›åç«¯"""
        backends = {
            "torch_sdpa": False,
            "xformers": XFORMERS_AVAILABLE,
            "flash_attention_2": False,
            "recommended": "torch",
        }
        
        # æ£€æŸ¥ PyTorch SDPA
        try:
            if hasattr(torch.nn.functional, "scaled_dot_product_attention"):
                backends["torch_sdpa"] = True
        except Exception:
            pass
        
        # æ£€æŸ¥ Flash Attention 2
        try:
            import flash_attn
            backends["flash_attention_2"] = True
        except ImportError:
            pass
        
        # æ¨èåç«¯
        if torch.cuda.is_available():
            cc = torch.cuda.get_device_capability()
            
            # SM80+ (A100, H100, RTX 30xx/40xx)
            if cc[0] >= 8:
                if XFORMERS_AVAILABLE and self.xformers_info.get("flash_attention"):
                    backends["recommended"] = "xformers"
                elif backends["flash_attention_2"]:
                    backends["recommended"] = "flash_attention_2"
                elif backends["torch_sdpa"]:
                    backends["recommended"] = "torch_sdpa"
            # SM70+ (V100, T4, RTX 20xx)
            elif cc[0] >= 7:
                if XFORMERS_AVAILABLE:
                    backends["recommended"] = "xformers"
                elif backends["torch_sdpa"]:
                    backends["recommended"] = "torch_sdpa"
            else:
                if backends["torch_sdpa"]:
                    backends["recommended"] = "torch_sdpa"
        
        logger.info(f"[RECOMMEND] Attention backend: {backends['recommended']}")
        return backends
    
    def _classify_gpu_tier(self, device_name: str, memory_total: float) -> str:
        """
        åŸºäºæ˜¾å­˜çš„ä¸¥æ ¼åˆ†çº§ (ç”¨æˆ·è¦æ±‚: 32G/24G/16G åˆ†çº§ï¼Œ<16G ä¸æ”¯æŒ)
        
        Args:
            device_name: GPU åç§°
            memory_total: æ˜¾å­˜å¤§å° (GB)
            
        Returns:
            str: 'tier_s', 'tier_a', 'tier_b', 'unsupported'
        """
        # é—¨æ§›ä¸‹è°ƒ 1Gï¼Œå› ä¸ºç³»ç»ŸæŠ¥å‘Šçš„æ˜¾å­˜ç•¥å°äºæ ‡ç§°å€¼ (å¦‚ 24G æŠ¥å‘Šä¸º 23.5G)
        if memory_total >= 31:
            return "tier_s"        # 32GBçº§ (A100/H100/Pro 6000/A6000/5090) - å…¨å¼€
        elif memory_total >= 23:
            return "tier_a"        # 24GBçº§ (3090/4090)
        elif memory_total >= 15:
            return "tier_b"        # 16GBçº§ (4080/4070TiS/A4000)
        else:
            return "unsupported"   # <15GB - ä¸æ”¯æŒ
    
    def get_optimized_config(self, config=None):
        """
        åŸºäºç¡¬ä»¶ç”ŸæˆåŠ¨æ€ä¼˜åŒ–é…ç½®
        """
        if config is None:
            config = {}
            
        memory_gb = self.gpu_info['memory_total']
        gpu_tier = self.gpu_info['gpu_tier']
        
        if gpu_tier == 'unsupported':
            logger.warning(f"[WARN] Detected VRAM ({memory_gb:.1f}GB) below minimum (16GB). Extreme mode enabled.")
            # æé™å‹æ¦¨æ¨¡å¼ï¼šå°è¯•ä½¿ç”¨æ›´å¤šæ˜¾å­˜ï¼Œè™½ç„¶é£é™©é«˜
            return {
                'mixed_precision': 'fp16',
                'gradient_checkpointing': True,
                'memory_efficient_preprocessing': True,
                'max_memory_mb': int(memory_gb * 1024 * 1.0), # 100% æ˜¾å­˜
                'spda_enabled': False,
                'block_swap_enabled': True,
                'block_swap_block_size': 1024, # å¢å¤§å—å¤§å°
                'block_swap_max_cache_blocks': int(memory_gb * 80), # å¢åŠ ç¼“å­˜å—
                'block_swap_swap_threshold': 0.99, # 99% é˜ˆå€¼ (æé™)
                'dataloader_num_workers': 4, 
            }

        # 1. ç¡®å®šæ··åˆç²¾åº¦ç±»å‹
        cc_major, cc_minor = self.gpu_info.get("compute_capability", (0, 0))
        cc = float(f"{cc_major}.{cc_minor}")
        use_bf16 = cc >= 8.0
        
        # åŸºç¡€é…ç½®
        optimized = {
            'mixed_precision': 'bf16' if use_bf16 else 'fp16',
            'gradient_checkpointing': True,
            'memory_efficient_preprocessing': True,
            'memory_monitoring_enabled': True,
            'comp_cache_compress': True,
        }
        
        # åŠ¨æ€è®¡ç®— max_memory_mb (æé™å‹æ¦¨: 100%)
        safe_memory_ratio = 1.0
        optimized['max_memory_mb'] = int(memory_gb * 1024 * safe_memory_ratio)
        
        # åŸºäº Tier çš„é…ç½®
        if gpu_tier == 'tier_s':
            # Tier S (32GB+: A100/H100/5090): å…¨æ€§èƒ½æ¨¡å¼
            # æ˜¾å­˜å……è£•ï¼Œä¸éœ€è¦ä»»ä½•å‹ç¼©æŠ€æœ¯
            optimized.update({
                # âŒ å…³é—­æ‰€æœ‰å‹ç¼©/äº¤æ¢åŠŸèƒ½
                'block_swap_enabled': False,
                'block_swap_block_size': 0,
                'block_swap_cpu_buffer_size': 0,
                'block_swap_swap_threshold': 0,
                
                # âŒ å…³é—­ SPDA - å•å¡æ— æ„ä¹‰
                'spda_enabled': False,
                
                # âœ… ä½¿ç”¨æœ€é«˜æ•ˆçš„æ³¨æ„åŠ›åç«¯
                'sdpa_enabled': True,
                'sdpa_flash_attention': True,
                'attention_backend': 'sdpa',
                
                # max_grad_norm ç”±ç”¨æˆ·åœ¨ toml ä¸­æŒ‡å®šï¼Œä¸è‡ªåŠ¨è¦†ç›–
                'dataloader_num_workers': 16,
                
                # xformers ä½œä¸ºå¤‡é€‰
                'xformers_enabled': self.xformers_info.get('available', False),
            })
            
        elif gpu_tier == 'tier_a':
            # Tier A (24GBçº§: 3090/4090): é«˜æ€§èƒ½æ¨¡å¼
            # 24GB è·‘ LoRA æ˜¯"å¯Œè£•ä»—"ï¼Œä¸éœ€è¦ Block Swap ç­‰æé™å‹ç¼©
            optimized.update({
                # âŒ å…³é—­ Block Swap - 24GB æ˜¾å­˜è¶³å¤Ÿï¼Œå¼€å¯åªä¼šæ‹–æ…¢é€Ÿåº¦
                'block_swap_enabled': False,
                'block_swap_block_size': 0,
                'block_swap_cpu_buffer_size': 0,
                'block_swap_swap_threshold': 0,
                
                # âŒ å…³é—­ SPDA - å•å¡æ— æ„ä¹‰ï¼Œåªå¢åŠ å¼€é”€
                'spda_enabled': False,
                
                # âœ… ä½¿ç”¨ PyTorch åŸç”Ÿ SDPA (è‡ªåŠ¨è°ƒç”¨ Flash Attention)
                'sdpa_enabled': True,
                'sdpa_flash_attention': True,
                'attention_backend': 'sdpa',  # ä¼˜å…ˆä½¿ç”¨åŸç”Ÿ SDPA
                
                # max_grad_norm ç”±ç”¨æˆ·åœ¨ toml ä¸­æŒ‡å®šï¼Œä¸è‡ªåŠ¨è¦†ç›–
                'dataloader_num_workers': 8,
                
                # xformers ä½œä¸ºå¤‡é€‰
                'xformers_enabled': self.xformers_info.get('available', False),
            })
            
        elif gpu_tier == 'tier_b':
            # Tier B (16GBçº§: 4080/4070TiS): å¹³è¡¡æ¨¡å¼
            # 16GB è·‘ LoRA ä»ç„¶è¶³å¤Ÿï¼Œä½†éœ€è¦æ›´ä¿å®ˆçš„é…ç½®
            optimized.update({
                # âš ï¸ 16GB å¯èƒ½éœ€è¦è½»åº¦ Block Swapï¼Œä½†é˜ˆå€¼ä¸è¦å¤ªæç«¯
                'block_swap_enabled': True,
                'block_swap_block_size': 512, 
                'block_swap_cpu_buffer_size': 2048, 
                'block_swap_swap_threshold': 0.85,  # 85% é˜ˆå€¼ï¼Œç•™ä¸€äº›ç¼“å†²
                'block_swap_prefetch_size': 128,
                'block_swap_swap_in_batch': 4,
                'block_swap_max_cache_blocks': int(memory_gb * 50), 
                'block_swap_prefetch_stream_count': 2,
                
                # âŒ å…³é—­ SPDA - å•å¡æ— æ„ä¹‰
                'spda_enabled': False,
                
                # âœ… ä½¿ç”¨åŸç”Ÿ SDPA
                'sdpa_enabled': True,
                'sdpa_flash_attention': True,
                'attention_backend': 'sdpa',
                
                # max_grad_norm ç”±ç”¨æˆ·åœ¨ toml ä¸­æŒ‡å®šï¼Œä¸è‡ªåŠ¨è¦†ç›–
                'dataloader_num_workers': 4,
                
                # xformers ä½œä¸ºå¤‡é€‰
                'xformers_enabled': self.xformers_info.get('available', False),
            })
            
        # SPDA World Size
        if optimized.get('spda_enabled', False):
            optimized['spda_world_size'] = torch.cuda.device_count()
        else:
            optimized['spda_world_size'] = 1
            
        # è¾“å‡ºä¼˜åŒ–çš„é…ç½®
        # æ ¹æ® tier æ˜¾ç¤ºä¸åŒçš„æ¨¡å¼æè¿°
        mode_desc = {
            'tier_s': 'Full Performance (no compression)',
            'tier_a': 'High Performance (LoRA optimized)',
            'tier_b': 'Balanced (light block swap)',
            'unsupported': 'Extreme (risky)'
        }.get(gpu_tier, 'Unknown')
        logger.info(f"[CONFIG] Hardware tier: {gpu_tier.upper()} (VRAM: {memory_gb:.1f}GB) - {mode_desc}")
        for key, value in optimized.items():
            logger.info(f"   {key}: {value}")
        
        return optimized
    
    def print_detection_summary(self):
        """æ‰“å°ç¡¬ä»¶æ£€æµ‹æ‘˜è¦"""
        # Use logger instead of print to avoid Windows GBK encoding issues
        logger.info("")
        logger.info("=" * 60)
        logger.info("[Hardware Detection Report]")
        logger.info("=" * 60)
        logger.info(f"GPU: {self.gpu_info['device_name']}")
        logger.info(f"VRAM: {self.gpu_info['memory_total']:.1f}GB")
        logger.info(f"GPU Tier: {self.gpu_info['gpu_tier']}")
        logger.info(f"CPU: {self.cpu_info['count']} cores")
        logger.info(f"System Memory: {self.memory_info['total']:.1f}GB")
        logger.info(f"Available Memory: {self.memory_info['available']:.1f}GB")
        logger.info("-" * 60)
        logger.info("Attention Backend:")
        xf_status = f"[OK] {self.xformers_info.get('version', '')}" if self.xformers_info.get('available') else "[NO]"
        logger.info(f"  xformers: {xf_status}")
        logger.info(f"  PyTorch SDPA: {'[OK]' if self.attention_info.get('torch_sdpa') else '[NO]'}")
        logger.info(f"  Flash Attention 2: {'[OK]' if self.attention_info.get('flash_attention_2') else '[NO]'}")
        logger.info(f"  Recommended: {self.attention_info.get('recommended', 'torch')}")
        logger.info("=" * 60)