"""
ğŸ’¾ å†…å­˜ä¼˜åŒ–å·¥å…·æ¨¡å—
æä¾›å—äº¤æ¢æŠ€æœ¯ã€æ¿€æ´»æ£€æŸ¥ç‚¹å’Œå†…å­˜ç®¡ç†åŠŸèƒ½

ä¸»è¦åŠŸèƒ½ï¼š
1. åŠ¨æ€å†…å­˜å—äº¤æ¢ (Block Swapping)
2. æ¿€æ´»å€¼æ£€æŸ¥ç‚¹ç®¡ç† (Activation Checkpointing)
3. CPU-GPUå†…å­˜äº¤æ¢ (CPU-GPU Memory Swapping)
4. å†…å­˜ç›‘æ§å’Œä¼˜åŒ– (Memory Monitoring)
"""

import torch
import gc
import psutil
import threading
import time
from typing import Dict, List, Optional, Any, Callable, Tuple
from collections import defaultdict, deque
from dataclasses import dataclass
from contextlib import contextmanager
import logging

logger = logging.getLogger(__name__)


@dataclass
class MemoryBlock:
    """å†…å­˜å—æ•°æ®ç»“æ„"""
    tensor_id: int
    tensor: torch.Tensor
    priority: float
    size_bytes: int
    created_at: float
    last_accessed: float
    access_count: int = 0
    
    def update_access(self):
        """æ›´æ–°è®¿é—®ä¿¡æ¯"""
        self.last_accessed = time.time()
        self.access_count += 1


class MemoryPool:
    """å†…å­˜æ± ç®¡ç†å™¨"""
    
    def __init__(self, strategy: str = "conservative"):
        self.strategy = strategy
        self.pools = {
            "gpu": {},
            "cpu": {}
        }
        self.total_allocated = 0
        self.max_pool_size = 0
        
        # æ ¹æ®ç­–ç•¥è®¾ç½®æ± å¤§å°
        if strategy == "conservative":
            self.max_pool_size = 1024 * 1024 * 1024  # 1GB
        elif strategy == "aggressive":
            self.max_pool_size = 4 * 1024 * 1024 * 1024  # 4GB
        
    def allocate(self, size_bytes: int, device: str = "gpu") -> bool:
        """åˆ†é…å†…å­˜æ± ç©ºé—´"""
        if self.total_allocated + size_bytes <= self.max_pool_size:
            self.total_allocated += size_bytes
            return True
        return False
    
    def deallocate(self, size_bytes: int):
        """é‡Šæ”¾å†…å­˜æ± ç©ºé—´"""
        self.total_allocated = max(0, self.total_allocated - size_bytes)


class BlockSwapManager:
    """å—äº¤æ¢ç®¡ç†å™¨"""
    
    def __init__(self, 
                 block_size: int = 512,
                 cpu_buffer_size_gb: float = 8.0,
                 swap_threshold: float = 0.7,
                 swap_frequency: int = 0,
                 smart_prefetch: bool = True,
                 swap_strategy: str = "priority",
                 compressed_swap: bool = False):
        
        self.block_size = block_size
        self.cpu_buffer_size = int(cpu_buffer_size_gb * 1024 * 1024 * 1024)  # è½¬æ¢ä¸ºå­—èŠ‚
        self.swap_threshold = swap_threshold
        self.swap_frequency = swap_frequency
        self.smart_prefetch = smart_prefetch
        self.swap_strategy = swap_strategy
        self.compressed_swap = compressed_swap
        
        # å†…å­˜å—ç®¡ç†
        self.gpu_blocks: Dict[int, MemoryBlock] = {}
        self.cpu_blocks: Dict[int, MemoryBlock] = {}
        self.block_counter = 0
        
        # è®¿é—®ç»Ÿè®¡
        self.access_patterns = defaultdict(deque)
        self.swap_history = deque(maxlen=1000)
        
        # å†…å­˜æ± 
        self.memory_pool = MemoryPool()
        
        # ç›‘æ§çº¿ç¨‹
        self.monitoring = False
        self.monitor_thread = None
        
        # é¢„å–é¢„æµ‹å™¨
        if self.smart_prefetch:
            self.prefetch_queue = deque()
            self.prediction_model = self._init_prediction_model()
    
    def _init_prediction_model(self):
        """åˆå§‹åŒ–ç®€å•çš„é¢„å–é¢„æµ‹æ¨¡å‹"""
        # è¿™é‡Œå¯ä»¥é›†æˆæ›´å¤æ‚çš„é¢„å–ç®—æ³•
        # ç›®å‰ä½¿ç”¨åŸºäºè®¿é—®é¢‘ç‡çš„ç®€å•é¢„æµ‹
        return {
            "hot_blocks": set(),
            "access_counts": defaultdict(int),
            "last_predictions": deque(maxlen=50)
        }
    
    def start_monitoring(self):
        """å¯åŠ¨å†…å­˜ç›‘æ§"""
        if not self.monitoring:
            self.monitoring = True
            self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
            self.monitor_thread.start()
            logger.info("ğŸ” å—äº¤æ¢å†…å­˜ç›‘æ§å·²å¯åŠ¨")
    
    def stop_monitoring(self):
        """åœæ­¢å†…å­˜ç›‘æ§"""
        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)
        logger.info("ğŸ›‘ å—äº¤æ¢å†…å­˜ç›‘æ§å·²åœæ­¢")
    
    def _monitor_loop(self):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                self._check_memory_usage()
                time.sleep(0.1)  # 100msæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.warning(f"å†…å­˜ç›‘æ§é”™è¯¯: {e}")
    
    def _check_memory_usage(self):
        """æ£€æŸ¥GPUå†…å­˜ä½¿ç”¨ç‡å¹¶æ‰§è¡Œäº¤æ¢"""
        if not torch.cuda.is_available():
            return
        
        # è·å–GPUå†…å­˜ä½¿ç”¨ç‡
        memory_info = torch.cuda.memory_stats(0)
        allocated = memory_info.get('allocated_bytes.all.current', 0)
        reserved = memory_info.get('reserved_bytes.all.current', 0)
        total = torch.cuda.get_device_properties(0).total_memory
        
        # è®¡ç®—ä½¿ç”¨ç‡
        usage_ratio = max(allocated, reserved) / total
        
        if usage_ratio > self.swap_threshold:
            logger.debug(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨ç‡ {usage_ratio:.2%} è¶…è¿‡é˜ˆå€¼ {self.swap_threshold:.2%}ï¼Œæ‰§è¡Œäº¤æ¢")
            self._perform_swap(usage_ratio - self.swap_threshold)
    
    def _perform_swap(self, excess_ratio: float):
        """æ‰§è¡Œå†…å­˜äº¤æ¢"""
        # ç¡®å®šéœ€è¦äº¤æ¢çš„å†…å­˜å—æ•°é‡
        target_blocks = int(len(self.gpu_blocks) * excess_ratio / self.swap_threshold)
        target_blocks = max(1, min(target_blocks, len(self.gpu_blocks) // 2))
        
        # æ ¹æ®ç­–ç•¥é€‰æ‹©è¦äº¤æ¢çš„å—
        blocks_to_swap = self._select_blocks_for_swap(target_blocks)
        
        for block_id in blocks_to_swap:
            if block_id in self.gpu_blocks:
                self._swap_out_block(block_id)
    
    def _select_blocks_for_swap(self, count: int) -> List[int]:
        """æ ¹æ®ç­–ç•¥é€‰æ‹©è¦äº¤æ¢çš„å†…å­˜å—"""
        if not self.gpu_blocks:
            return []
        
        if self.swap_strategy == "fifo":
            # å…ˆè¿›å…ˆå‡ºç­–ç•¥
            sorted_blocks = sorted(self.gpu_blocks.keys(), 
                                 key=lambda x: self.gpu_blocks[x].created_at)
        
        elif self.swap_strategy == "lru":
            # æœ€è¿‘æœ€å°‘ä½¿ç”¨ç­–ç•¥
            sorted_blocks = sorted(self.gpu_blocks.keys(), 
                                 key=lambda x: self.gpu_blocks[x].last_accessed)
        
        elif self.swap_strategy == "priority":
            # åŸºäºä¼˜å…ˆçº§çš„ç­–ç•¥ (ä¿ç•™é‡è¦å—)
            sorted_blocks = sorted(self.gpu_blocks.keys(), 
                                 key=lambda x: self.gpu_blocks[x].priority)
        
        else:
            sorted_blocks = list(self.gpu_blocks.keys())
        
        return sorted_blocks[:count]
    
    def _swap_out_block(self, block_id: int):
        """äº¤æ¢å‡ºå†…å­˜å—åˆ°CPU"""
        if block_id not in self.gpu_blocks:
            return
        
        block = self.gpu_blocks[block_id]
        
        # æ£€æŸ¥CPUç¼“å†²åŒºç©ºé—´
        current_cpu_usage = sum(b.size_bytes for b in self.cpu_blocks.values())
        if current_cpu_usage + block.size_bytes > self.cpu_buffer_size:
            # CPUç¼“å†²åŒºæ»¡ï¼Œæ¸…ç†æœ€æ—§çš„å—
            self._evict_cpu_blocks()
        
        # ç§»åŠ¨åˆ°CPU
        cpu_tensor = block.tensor.cpu().detach()
        self.cpu_blocks[block_id] = MemoryBlock(
            tensor_id=block.tensor_id,
            tensor=cpu_tensor,
            priority=block.priority,
            size_bytes=block.size_bytes,
            created_at=block.created_at,
            last_accessed=block.last_accessed,
            access_count=block.access_count
        )
        
        # ä»GPUåˆ é™¤
        del self.gpu_blocks[block_id].tensor
        del self.gpu_blocks[block_id]
        
        # è®°å½•äº¤æ¢å†å²
        self.swap_history.append({
            "block_id": block_id,
            "direction": "out",
            "timestamp": time.time(),
            "size": block.size_bytes
        })
        
        # è§¦å‘åƒåœ¾å›æ”¶
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    def _swap_in_block(self, block_id: int):
        """äº¤æ¢å…¥å†…å­˜å—åˆ°GPU"""
        if block_id not in self.cpu_blocks:
            return
        
        block = self.cpu_blocks[block_id]
        
        # æ£€æŸ¥GPUç©ºé—´ (ä½¿ç”¨é…ç½®çš„é˜ˆå€¼)
        if torch.cuda.is_available():
            # å…è®¸ä½¿ç”¨åˆ° swap_threshold çš„ä¸Šé™
            if torch.cuda.memory_reserved(0) > torch.cuda.get_device_properties(0).total_memory * self.swap_threshold:
                logger.warning(f"âš ï¸ GPUå†…å­˜ä¸è¶³ (>{self.swap_threshold:.1%})ï¼Œæ— æ³•äº¤æ¢å…¥å—")
                return
        
        # ç§»åŠ¨åˆ°GPU
        gpu_tensor = block.tensor.cuda().detach()
        self.gpu_blocks[block_id] = MemoryBlock(
            tensor_id=block.tensor_id,
            tensor=gpu_tensor,
            priority=block.priority,
            size_bytes=block.size_bytes,
            created_at=block.created_at,
            last_accessed=block.last_accessed,
            access_count=block.access_count
        )
        
        # ä»CPUåˆ é™¤
        del self.cpu_blocks[block_id]
        
        # è®°å½•äº¤æ¢å†å²
        self.swap_history.append({
            "block_id": block_id,
            "direction": "in",
            "timestamp": time.time(),
            "size": block.size_bytes
        })
    
    def _evict_cpu_blocks(self):
        """æ¸…ç†CPUå—ï¼Œè…¾å‡ºç©ºé—´"""
        if not self.cpu_blocks:
            return
        
        # æŒ‰ä¼˜å…ˆçº§æ¸…ç†CPUå—
        sorted_blocks = sorted(self.cpu_blocks.keys(), 
                             key=lambda x: self.cpu_blocks[x].priority)
        
        # æ¸…ç†10%æˆ–æœ€å°‘1ä¸ªå—
        evict_count = max(1, len(sorted_blocks) // 10)
        for block_id in sorted_blocks[:evict_count]:
            del self.cpu_blocks[block_id]
    
    def register_tensor(self, tensor: torch.Tensor, priority: float = 1.0) -> int:
        """æ³¨å†Œå¼ é‡åˆ°å—ç®¡ç†å™¨"""
        block_id = self.block_counter
        self.block_counter += 1
        
        # è®¡ç®—å—å¤§å°
        size_bytes = tensor.numel() * tensor.element_size()
        
        # åˆ›å»ºå†…å­˜å—
        memory_block = MemoryBlock(
            tensor_id=block_id,
            tensor=tensor,
            priority=priority,
            size_bytes=size_bytes,
            created_at=time.time(),
            last_accessed=time.time()
        )
        
        self.gpu_blocks[block_id] = memory_block
        
        logger.debug(f"ğŸ“ æ³¨å†Œå¼ é‡å— {block_id}ï¼Œå¤§å°: {size_bytes / 1024 / 1024:.2f}MB")
        return block_id
    
    def update_tensor_access(self, block_id: int):
        """æ›´æ–°å¼ é‡è®¿é—®ä¿¡æ¯"""
        if block_id in self.gpu_blocks:
            self.gpu_blocks[block_id].update_access()
        elif block_id in self.cpu_blocks:
            self.cpu_blocks[block_id].update_access()
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "gpu_blocks": len(self.gpu_blocks),
            "cpu_blocks": len(self.cpu_blocks),
            "total_gpu_memory": sum(b.size_bytes for b in self.gpu_blocks.values()),
            "total_cpu_memory": sum(b.size_bytes for b in self.cpu_blocks.values()),
            "swap_operations": len(self.swap_history),
            "memory_pool_usage": self.memory_pool.total_allocated
        }
        
        if torch.cuda.is_available():
            stats["gpu_memory_usage"] = torch.cuda.memory_allocated(0)
            stats["gpu_memory_reserved"] = torch.cuda.memory_reserved(0)
            stats["gpu_memory_total"] = torch.cuda.get_device_properties(0).total_memory
        
        return stats


class ActivationCheckpointManager:
    """æ¿€æ´»æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, optimization_level: str = "basic"):
        self.optimization_level = optimization_level
        self.checkpoint_cache = {}
        self.computation_graph = {}
        
    @contextmanager
    def checkpoint_context(self, module_name: str):
        """æ£€æŸ¥ç‚¹ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if self.optimization_level == "none":
            yield
            return
        
        start_time = time.time()
        
        try:
            if self.optimization_level == "aggressive":
                # æ¿€è¿›æ£€æŸ¥ç‚¹ï¼šä¿å­˜æ›´å¤šä¸­é—´çŠ¶æ€
                torch.cuda.empty_cache()
                gc.collect()
            
            yield
            
        finally:
            computation_time = time.time() - start_time
            logger.debug(f"ğŸ”„ æ¨¡å— {module_name} æ£€æŸ¥ç‚¹è®¡ç®—è€—æ—¶: {computation_time:.3f}s")
    
    def create_checkpoint(self, forward_fn: Callable, *args, **kwargs):
        """åˆ›å»ºæ£€æŸ¥ç‚¹"""
        if self.optimization_level == "none":
            return forward_fn(*args, **kwargs)
        
        if self.optimization_level == "basic":
            return torch.utils.checkpoint.checkpoint(forward_fn, *args, **kwargs)
        
        elif self.optimization_level == "aggressive":
            # æ¿€è¿›æ£€æŸ¥ç‚¹ï¼šè‡ªå®šä¹‰é‡è®¡ç®—é€»è¾‘
            return torch.utils.checkpoint.checkpoint_sequential(
                forward_fn, 
                segments=2,  # åˆ†æ®µæ£€æŸ¥ç‚¹
                *args, 
                **kwargs
            )
        
        return forward_fn(*args, **kwargs)


class MemoryOptimizer:
    """ç»Ÿä¸€å†…å­˜ä¼˜åŒ–ç®¡ç†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        
        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.block_swap = BlockSwapManager(
            block_size=config.get('memory_block_size', 512),
            cpu_buffer_size_gb=config.get('cpu_swap_buffer_size', 8.0),
            swap_threshold=config.get('swap_threshold', 0.7),
            swap_frequency=config.get('swap_frequency', 0),
            smart_prefetch=config.get('smart_prefetch', True),
            swap_strategy=config.get('swap_strategy', 'priority'),
            compressed_swap=config.get('compressed_swap', False)
        )
        
        self.checkpoint_manager = ActivationCheckpointManager(
            optimization_level=config.get('checkpoint_optimization', 'basic')
        )
        
        self.enabled = config.get('block_swap_enabled', True)
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            "total_saves": 0,
            "total_swaps": 0,
            "avg_swap_time": 0.0,
            "memory_efficiency": 0.0
        }
    
    def start(self):
        """å¯åŠ¨å†…å­˜ä¼˜åŒ–å™¨"""
        if self.enabled:
            self.block_swap.start_monitoring()
            logger.info("ğŸ’¾ å—äº¤æ¢å†…å­˜ä¼˜åŒ–å™¨å·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢å†…å­˜ä¼˜åŒ–å™¨"""
        if self.enabled:
            self.block_swap.stop_monitoring()
            logger.info("ğŸ›‘ å—äº¤æ¢å†…å­˜ä¼˜åŒ–å™¨å·²åœæ­¢")
    
    def optimize_training_step(self):
        """è®­ç»ƒæ­¥éª¤å†…å­˜ä¼˜åŒ–"""
        if not self.enabled:
            return
        
        # ä»…åœ¨å†…å­˜æåº¦ç´§å¼ æ—¶æ¸…ç†ç¼“å­˜ (ä¾‹å¦‚ > 95%)
        # é¢‘ç¹æ¸…ç†ä¼šä¸¥é‡å½±å“æ€§èƒ½
        if torch.cuda.is_available():
            usage = torch.cuda.memory_allocated(0) / torch.cuda.get_device_properties(0).total_memory
            if usage > 0.98:
                torch.cuda.empty_cache()
                gc.collect()
        
        # æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        stats = self.block_swap.get_memory_stats()
        self._update_performance_stats(stats)
    
    def _update_performance_stats(self, stats: Dict[str, Any]):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        total_blocks = stats["gpu_blocks"] + stats["cpu_blocks"]
        if total_blocks > 0:
            self.stats["memory_efficiency"] = (
                stats["gpu_blocks"] / total_blocks
            ) * 100
    
    def get_detailed_stats(self) -> Dict[str, Any]:
        """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        block_stats = self.block_swap.get_memory_stats()
        
        # ç³»ç»Ÿå†…å­˜ä¿¡æ¯
        system_memory = psutil.virtual_memory()
        
        return {
            "block_swap": block_stats,
            "checkpoint": {
                "optimization_level": self.checkpoint_manager.optimization_level,
                "cached_checkpoints": len(self.checkpoint_manager.checkpoint_cache)
            },
            "system_memory": {
                "total": system_memory.total,
                "available": system_memory.available,
                "percent": system_memory.percent
            },
            "performance": self.stats
        }